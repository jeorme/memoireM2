%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Wenneker Assignment
% LaTeX Template
% Version 2.0 (12/1/2019)
%
% This template originates from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Vel (vel@LaTeXTemplates.com)
% Frits Wenneker
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt]{scrartcl} % Font size
\usepackage{hyperref} % for url

\input{structure.tex} % Include the file specifying the document structure and custom commands
\newtheorem{theorem}{Théorème}[section]
\newtheorem{corollary}{Corollaire}[theorem]
\newtheorem{lemma}[theorem]{Lemme}
\newtheorem{Def}[theorem]{Definition}
\newtheorem{pro}[theorem]{Proposition}

%%newcommand
\newcommand{\Xt}{\left(X_t\right)_{t\in\mathbb{Z}}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\var}[1]{\mathbb{V}\left(#1\right)}
\newcommand{\E}[1]{\mathbb{E}\left(#1\right)}
\newcommand{\bb}[1]{\mathcal{BB}\left(0,#1\right)}
\newcommand{\bbn}[1]{\mathcal{BBN}\left(0,#1\right)}
%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{	
	\normalfont\normalsize
	\textsc{CNAM}\\ % Your university, school and/or department name(s)
	\vspace{25pt} % Whitespace
	\rule{\linewidth}{0.5pt}\\ % Thin top horizontal rule
	\vspace{20pt} % Whitespace
	{\huge Series temporelles : ARIMA, LSTM?}\\ % The assignment title
	\vspace{12pt} % Whitespace
	\rule{\linewidth}{2pt}\\ % Thick bottom horizontal rule
	\vspace{12pt} % Whitespace
}

\author{\LARGE Jérôme Petit} % Your name

\date{\normalsize\today} % Today's date (\today) or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	FIGURE EXAMPLE
%----------------------------------------------------------------------------------------
%
\section{Introduction}


\section{Séries temporelles}
Une série temporelle ou série chronologique est une suite d'observations d’un phénomène faites au cours du temps. Cette notion s'étend à de nombreux domaines, tout relevé de données, de flux ou d'information est une série temporelle : indice boursier, population française, trafic Internet, etc. Ses données réelles obtenues sont des suites finies indexés par un temps continu ou discret. Les modèles mathématiques utilisées pour modélisées ses séries temporelles sont eux de dimension infinie.


Dans la suite de ce document, on notera l'observation $x_t$ pour tout $t=1,...,T$ d'une variable aléatoire $X_t(\omega)$. Si le processus a été spécifié, estimé et validé, on peut alors l'utiliser pour effectuer une prévision. On notera alors $\hat{X}_T(h)$ le prédicteur de la variable aléatoire $X_{T+h}$. Ce prédicteur permettra de calculer la prévision $\hat{x}_T(h)$ comme étant la réalisation de $\hat{X}_T(h)$. 


Afin d'étudier les séries temporelles, nous allons utiliser les moment d'ordre 1 et 2. Le moment d'ordre 1 permet de déterminer la tendance d'un processus et les moments d'ordre 2 permet de déterminer la variance du processus et d'étudier la dépendance temporelle du processus.
\begin{Def}\label{defMom}
Soit un processus $\Xt$ un processus du second ordre (i.e. $\E{X_t^2}<\infty$).
\begin{itemize}
\item[i)]La fonction moyenne $m$ du processus $\Xt$ est l'espérance non conditionnelle du processus~:~$m(t)=\E{X_t}$, $\forall t\in \Z$.
\item[ii)]La fonction d'autocovaraince de retard $k$, notée $\gamma(k)$, du processus $\Xt$ est définie de la manière suivante, $\forall t, k\in\Z$~:
$$
\gamma(k)=cov\left(X_t,X_{t+k}\right)=\E{\left(X_t-\E{X_t}\right)\left(X_{t+k}-\E{X_{t+k}}\right)}
$$
\item[iii)]La fonction d'autocorrélation de retard $k$ (ACF), notée $\rho(k)$, du processus $\Xt$ est définie de la manière suivante, $\forall t, k\in\Z$~:
$$
\rho(k)=\frac{\gamma(k)}{\sigma_{X_t}\sigma_{X_{t+k}}}
$$
avec $\sigma_{X_t}$ l'écart type du processus au temps $t$~:~$\sigma_{X_t}=\sqrt{\gamma(0)}$
\item[iv)]La fonction d'autocorrélation partielle de retard $k$ (PACF), notée $r(k)$, du processus $\Xt$ est définie de la manière suivante, $\forall t, k\in\Z$~:
$$
r(k)=\frac{cov(X_t-X_t^*,X_{t+k}-X_{t+k}^*)}{\sqrt{var(X_t-X_t^*)}\sqrt{var(X_{t+k}-X_{t+k}^*)}}
$$
avec $X_t^*$ la régression affine de $X_t$ sur $X_{t+1},X_{t+2},..., X_{t+k-1}$ et $X^*_{t+k}$ est la régression affine de $X_{t+k}$ sur $X_{t+k-1},X_{t+k-2},...,X_{t+1}$.
\end{itemize}
\end{Def}
On dit que le processus est centré si pour tout t $m(t)=0$. Le coefficient $r(k)$ mesure la liaison entre les variables $X_t$ et $X_{t+k}$ une fois que l'on a retranché l'influence des variables intermédiaires. On peut exprimer le terme d'autocorellation partielle à l'aide d'un régression linéaire.
\begin{pro}
Le coefficient $r(k)$ définit dans (\ref{defMom}) est le coefficient de $X_t$ dans la régression linéaire de $X_{t+k}$ sur 1, $X_t$, $X_{t+1}$,..., $X_{t+k-1}$.
\end{pro}

La définition de l'autocorelation partielle peut s'écrire à l'aide de l'auto correlation~:
$$
\left(
\begin{tabular}{c}
$c_1$\\
$c_2$\\
. \\
. \\
r(k)
\end{tabular}
\right) = 
\left(
\begin{tabular}{cccc}
1 & $\rho(1)$ & . & $\rho(k-1)$\\
$\rho(1)$ & 1 & . & $\rho(k-2)$\\
. & . & . & . \\
. & . & . & . \\
$\rho(k-1)$ & $\rho(k-2)$ & . & 1
\end{tabular}
\right)^{-1}
\left(
\begin{tabular}{c}
$\rho(1)$\\
$\rho(2)$\\
. \\
. \\
$\rho(k)$
\end{tabular}
\right)
$$



Dans la pratique, on aura un échantillon fini, il faudra donc estimer ses moments à partir de l'échantillon fourni. Dans la suite on appelera ses estimations des \textit{moments empiriques}~:
\begin{align*}
\overline{X}_T &= \frac{1}{T}\sum_{t=1}^TX_t\\
\overline{\gamma}(h)&=\frac{1}{T-h}\sum_{t=h+1}^T(X_t-\overline{X})(X_{t-h}-\overline{X}), 0\leq h\leq T-1\\
\overline{\rho}(h)&=\frac{T}{T-h}\frac{\sum_{t=h+1}^T(y_t-\overline{y})(y_{t-h}-\overline{y})}{\sum_{t=1}^T(y_t-\overline{y})^2}, 0\leq h\leq T-1,\\
\end{align*}
en remplaçant $T-h$ par $T$ l'autocorellation on obtient un estimateur avec biais.
\subsection{Densité spectrale}
Les coefficients d'autocovariance d'une série stationnaire correspondent aux coefficients de Fourier d'une mesure positive, appelée mesure spectrale du processus. Il est possible de montrer que cette mesure spectrale admet un densité, dite spectrale, par rapport à la mesure de Lebesgues sur $[-\pi,\pi]$ que nous noterons $f_X$.
\begin{Def}
Soit $\Xt$ un processus de fonction d'autocovariance $\gamma_X$ la densité spectrale $f$ de $\Xt$ s'écrit~:
$$
f_X(\omega) = \frac{1}{2\pi}\sum_{h\in\mathbb{Z}}\gamma_X(h)e^{-i\omega h}\,.
$$
\end{Def}
\begin{pro}
Si $f_X$ est la densité spectrale de $\Xt$ alors~:
$$
\gamma_X(h)=\int_{-\pi}^{\pi}f_X(\omega)e^{i\omega h}d\omega
$$
\end{pro}
Le principal outil d'analyse dont on dispose pour estimer empiriquement la densité spectrale théorique du processus est le périodogramme $I_T$, défini sur l'intervalle $[0,2\pi[$ par~:
$$
I_T(\lambda)=\frac{1}{2\pi T}\left|\sum_{t=1}^Te^{-i\lambda t}X_t\right|^2
$$

\subsection{processus à mémoire}
L'ACF permet de mesurer la persistence (ou la mémoire) d'un processus, elle va pouvoir classer les processus en fonction de leur mémoire. On va considérer les processus sans mémoire, à mémoire courte et à mémoire longue.


Un \textit{processus sans mémoire} est un processus tel que pour tout $t$ $X_t$ n'est pas corrélé à $X_{t-1}, X_{t-2},...$. Les bruits blancs sont des processus sans mémoire.


\begin{Def}\label{BB}
Un bruit blanc $\epsilon_t$ est une suite de variable aléatoire non corrélées, mais pas nécessairement indépendantes de moyenne nulle et de variance constante $\sigma^2$. On note $\epsilon_t\sim \bb{\sigma^2}$.
\newline 
Un \textit{bruit blanc fort} est un bruit blanc dont les variables aléatoires sont indépendantes.
\end{Def}
Pour un bruit blanc $X_t$, on a~:
\begin{itemize}
\item $\E{X_t}=0$
\item $\gamma(h)=0$ si $h\not =0$
\end{itemize}
De la même manière on définit~:
\begin{Def}\label{BBG}
Un bruit blanc gaussien $\epsilon_t$ est une suite de variable aléatoire i.i.d de loi normale $\mathcal{N}\left(0,\sigma^2\right)$. On note $\epsilon\sim \bbn{\sigma^2}$
\end{Def}
Un bruit blanc gaussien est également un processus sans mémoire.


La dénomination de \textit{bruit} vient du fait que ce processus ne contient aucune information d'autocorrelation. Ainsi aucun signal déterministe ne peut être extrait de ce processus. 


Il existe également des processus dont l'ACF est géométriquement bornée et décroit rapidement vers 0, on parle alors de processus à mémoire courte. C'est le cas des processus de ARMA, AR ou MA.
\begin{Def}\label{defCourte}
Un processus est dit à mémoire courte s'il possède une ACF, $\rho(k)$, telle que~:
$$
\rho(k)\leq Cr^{k}
$$
avec $C>0$ et $0<r<1$
\end{Def}
Par exemple, les modèles MA(1) et AR(1) sont des processus à mémoire courte. En effet, pour un processus $X_t$ qui est un processus MA(1), on a~:~$X_t=\epsilon_t+\theta\epsilon_{t-1}$ avec $\epsilon_t$ un bruit blanc de variance $\sigma^2$, on a~:
\begin{align*}
\E{X_t}&=0\\
\gamma(0)&=\sigma^2(1+\theta^2)\\
\gamma(1) &= \theta\sigma^2\\
\gamma(h)&= 0, \forall h\not=0,1
\end{align*}
ainsi $\rho(h)=0$ pour tout $h>1$.
\begin{Def}
Un processus est dit à mémoire longue s'il possède une ACF, $\rho(k)$, qui est approchée comme suit~:
$$
\rho(k)\sim_{k\rightarrow \infty} Ck^{-\alpha}
$$
avec $C>0$ une constante et $\alpha\in ]0,1[$.
\end{Def}
Pour des processus à mémoire longue la série des autocorellations est absolument divergente. Les processus fractionnaires de type ARFIMA permettent de reproduire ce comportement.

\subsection{Stationnarités}
\subsubsection{Stationnarités faibles et fortes}
\begin{Def}\label{staForte}
Stationnarité stricte ou forte~:~$\left(X_t\right)_{t\in\mathbb{Z}}$ est un processus stationnaire au sens strict si~:~$\forall n\in \mathbb{N}, \forall (t_1,...,t_n), \forall h\in\mathbb{Z}$ la loi de $(X_{t_1},...,X_{t_n})$ est identique à la loi de $(X_{t_1+h},...,X_{t_n+h})$
\end{Def}
D'après le théorème de Kolmogorov, on en déduit qu'un processus $\left(X_t\right)$ est stationnaire au sens fort si et seulement si la loi de $X_t$ est identique à la loi de $X_{t+h}$ quelque soit $h$. Cette condition de stationnarité est très contraignante et relativement peu observée en pratique. On remarque que pour un processus stationnaire strict $X_t$ admettant des moments d'ordre 1 et 2 finis, on a~:
$$
\mathbb{E}\left(X_t\right)=\mathbb{E}\left(X_{t+h}\right), \forall h\in \mathbb{N}
$$
on pose~:~$\mathbb{E}\left(X_t\right)=\mathbb{E}\left(X_0\right)=\mu$. Pour la covariance, on a~:
\begin{align*}
Cov\left(X_t,X_{t+h}\right)& = \mathbb{E}\left((X_t-\mu)(X_{t+h}-\mu)\right)\\
&= \mathbb{E}\left((X_0-\mu)(X_{h_0}-\mu)\right)
\end{align*}
la dernière égalité vient du fait que $(X_t,X_{t+h})$ a la même distribution que $(X_0,X_h)$.
On remarque donc que les processus stationnaire strict ont des moments d'ordre 1 et 2 indépendant du temps $t$. C'est cette notion qui va être utilisée pour des définir les processus stationnaires au sens faible.
\begin{Def}\label{staFaible}
Stationnarité faible ou du second ordre~:~
$\left(X_t\right)_{t\in\mathbb{Z}}$ est un processus stationnaire du second ordre (ou processus faiblement stationnaire ) s'il vérifie~:
\begin{itemize}
\item[i)]$\forall t\in\mathbb{Z},~\mathbb{E}\left(X_t\right)=m $ 
\item[ii)]$\forall t\in\mathbb{Z},~\mathbb{V}\left(X_t\right)=\sigma^2 $ 
\item[iii)]$\forall t,h\in\mathbb{Z},~Cov\left(X_t,X_{t+h}\right)=Cov\left(X_0,X_h\right)=\gamma\left(h\right) $ 
\end{itemize}
On note $\gamma(h)$ l'autocovariance d'ordre $h$ de $X_t$ .
\end{Def} 
Un processus fortement stationnaire est un processus stationnaire du second ordre. L'inverse n'est pas généralement vrai. Dans la suite les processus stationnaires désigneront les processus stationnaires du second ordre.


On peut généraliser la définition de processus stationnaire à l'ordre deux aux cas vectoriel~:
\begin{Def}\label{staFaibleVec}
Stationnarité vectorielle faible ou du second ordre~:~
$\left(X_t\right)_{t\in\mathbb{Z}}$ est un processus stationnaire vectoriel (n,1) du second ordre (ou processus faiblement stationnaire ) s'il vérifie~:
\begin{itemize}
\item[i)]$\forall t\in\mathbb{Z},~\mathbb{E}\left(X_t\right)=m $ 
\item[ii)]$\forall t,h\in\mathbb{Z},~\E{(X_t-m)(X_{t+h}-m)}=\gamma\left(h\right) $ est indépendant de $t$ 
\end{itemize}
\end{Def} 
Ainsi pour tout $h$ la matrice $\gamma(h)$ est une matrice carrée de taille n indépendante de t. Dans le cas des 

Les processus stationnaires sont stables par combinaison sous certaine condition sur les coefficients : 
\begin{pro}
Si $\Xt$ est un processus stationnaire et $(a_i)_{i\in \mathbb{Z}}$ est une suite de réels absolument convergente alors le processus : 
$$
Y_t=\sum_{i\in\mathbb{Z}}a_i X_{t-i}
$$
est un processus stationnaire
\end{pro}
Une conséquence de cette proposition est que la différence ou la moyenne de processus stationnaire est un processus stationnaire.



Pour un processus stationnaire, la fonction d'auto-correlation partielle satisfait la relation~:
$$
r(k) = \frac{\left|P_k^*\right|}{\left|P_k\right|}
$$
avec~:
$$
P_k=\left(\begin{tabular}{cccc}
1 & $\rho(1)$ & . & $\rho(k-1)$\\
$\rho(1)$ & 1 & . & $\rho(k-2)$\\
. & . & . & .\\
$\rho(k-1)$ & $\rho(k-2)$ & . & 1
\end{tabular}
\right)
$$
et 
$$
P_k^*=\left(\begin{tabular}{cccc}
1 & $\rho(1)$ & . & $\rho(1)$\\
$\rho(1)$ & 1 & . & $\rho(k-2)$\\
. & . & . & .\\
$\rho(k-1)$ & $\rho(k-2)$ & . & 1\\
\end{tabular}
\right)
$$
Les 3 premières autocrrélations partielles sont donc déterminées par les relation suivantes~:
\begin{align*}
r(1)&=\rho(1)\\
r(2)&= \frac{\rho(2)-\rho(1)^2}{1-\rho(1)^2}\\
r(3)&=\frac{\rho(1)^3-\rho(1)\rho(2)(2-\rho(2))+\rho(3)(1-\rho(1)^2)}{1-\rho(2)^2-2\rho(1)^2(1-\rho(2))}
\end{align*}
On peut estimer l'auto corrélation partielle par une méthodes des moindres carrées~:
$$
x_{t+1} = \hat{c} + \hat{r}(1)x_{t-1}+\hat{r}(2)x_{t-2}+...+\hat{r}(k)x_{t-k+1}\,,\forall k \in \Z
$$
A l'aide de la relation précédente $\hat{r}(k)=\frac{\left|P_k^*\right|}{\left| P_k \right|}$, l'estimation de $\hat{r}(k)$ permet d'obtenir les estimations de $\hat{\rho}$.



La stationnarité d'un processus permet d'estimer les moments non conditionnels de $X_{T+h}$ en utilisant les moments empiriques. Ainsi on peut utiliser comme prédicateur naturel de $X_{T+h}$ un estimateur de l'espérance non conditionnelle $\E{X_{T+h}}$ en particulier la moyenne empirique, i.e.~:~$\hat{X}_{T+h}=\frac{1}{T}\sum_{t=1}^TX_t$. Mais on se rend compte que cette prévision est extrêmement grossière car pour n'importe quel horizon $h>0$, le prédicateur est identique, illustrant ainsi que la dynamique du processus n'est pas prise en compte dans ce type de prédicateur. Ainsi une fois choisi le prédicateur il faut pouvoir déterminer la qualité de la prédiction.


\subsubsection{localement stationnaire}
Dahlaus a défini dans \cite{locally} la notion de stationnarité local pour les processus linéaire. Afin de la définir, j'ai introduit quelques notions nécessaires. La notion de localement stationnaire est utilisé pour des processus dont la représentation dépend du temps. Cela signifie, par example qu'un processus AR(1) dépendant du temps s'écrit : $X_t+\alpha_tX_{t-1}=\epsilon_t$, les coefficients du modèle dépendante du temps. On ne peut plus avoir de stationnarité globale car la valeur de se coefficient va dépendre de t. De plus sont estimation va dépendre de t et de la taille de l'échantillon T à l'instant t. Le paramètre du modèle dépendent de t et sont calculés sur l'échantillon $[1,T]$, afin d'éviter les problème d'estimation sur une taille finie et de passage à la lime quand $t\rightarrow \infty$, on réalise un changement d'échelle $\frac{t}{T}$ afin d'étudier sur l'intervalle $[0,1]$ et le passage à la limite se fera quand $t\rightarrow 1$. Dans \cite{locally}, la définition est donnée pour des processus linéaire (MA($\infty$)). Rappelons que la représentation d'un processus linéaire $X_{t,T}$ est~:
$$
X_{t,T}=\mu\left(\frac{t}{T}\right)+\sum_{j=-\infty}^\infty a_{t,T}(j)\epsilon_{t-j}
$$
L'approximation de se processus va reposer sur l'estimation de $a_{t,T}(j)\approx a(\frac{t}{T},j)$ et donc imposer certaine régularité sur le coefficient a. Les conditions sur a telles que pour tout $u_0$
Avec cette précision, l'idée principale de la stationnarité locale est que pour tout $u_0\in [0,1]$ le processus $X_{t,T}$ peut être approché par un processus stationnaire $\tilde{X}_t(u_0)$. Le processus stationnaire $\tilde{X}_t(u_0)$ s'écrit~:
$$
\tilde{X}_t(u)=\mu(u)+\sum_{j=-\infty}^\infty a(u,j)\epsilon_{t-j}
$$

\subsubsection{Non stationnarité}
La stationnarité d'ordre 2 se définie comme une invariance des moments d'ordre 1 et 2. Par opposition, on dira qu'une série est non-stationnaire si elle n'est pas stationnaire. La classe des processus non stationnaire est plus vaste que celle des processus stationnaire. Parmi les processus non stationnaire Nelson et Plosser \cite{NonSta} ont retenu deux classes de processus non-stationnaires : les processus TS \textit{(trend stationary)} et les processus DS \textit{(difference stationary)}. Les premiers correspondent à une non-stationnarité de type déterministe alors que les seconds correspondent à une non-stationnarité de type stochastique.
\begin{Def}
$\Xt$ est un processus non-stationnaire TS s'il peut s'écrire sous la forme $X_t = f(t)+Z_t$ où $f(t)$ est une fonction déterministe du temps et $Z_t$ est un processus stationnaire.
\end{Def}
L'exemple le plus simple est celui de la tendance linéaire bruitée~:~$X_t=a+bt+\epsilon_t$. Ce processus admet une tendance déterministe $a+bt$. Les processus non-stationnaire TS non pas de persistance des chocs. L'influence d'un choc subit à un instant $\tau$ aura tendance ) s'estomper au cours du temps et la variable rejoint alors sa dynamique à long terme déterminé par $f(t)$. Cette propriété une conséquence directe de la décomposition de Wold.
\begin{Def}
Un processus $\Xt$ est un processus non-stationnaire DS ou intégré d'ordre $d$ si le processus obtenu après $d$ différenciation est stationnaire.
\end{Def}
Le fait de différencier $d$ fois revient à multiplier par le polynôme de retard $(1-L)^d$ et cela revient donc à chercher les racines de l'unité. Car si le polynôme $\Phi(L)X_t$ est stationnaire et si 1 est une racine du polynôme $\Phi$ alors $\left(X_t\right)$ sera non stationnaire. C'est pour cela que les tests de détections de racine de l'unité sont utilisés pour la détection de non stationnarité.


\subsection{Prédiction de séries temporelles}
La prédiction des séries temporelles consiste à construire à partir des données $X_t$ avec $0\leq t\leq T$ une série $\hat{X}_T(h)$ tel que pour tout $h$ $X_{T+h}\approx \hat{X}_T(h)$. La qualité de la prédiction dépendra de la qualité de l'approximation pour tout h.


Les propriétés qui caractérisent un bon estimateur sont le fait d'être sans biais et de variance minimale. Afin d'étudier ses propriété on introduit la variable d'erreur de prévision à l'horizon h définie par~:
$$
e_{T+h} = X_{T+h}-\hat{X}_T(h)
$$
La caractéristique principale d'un bon prédicteur set de minimiser cette erreur de prévision au sens d'un certain critère. Généralement, 3 critères d'erreur de prévision à l'horizon h sont retenues~:~l'erreur moyenne \textit{ME}, l'erreur absolue moyenne \textit{MAE}, l'erreur quadratique moyenne \textit{MSE}~:
\begin{align*}
ME&=E\left(e_{T+h}\right)\\
MAE&=E\left(\left|e_{T+h}\right|\right)\\
MSE&=E\left(e_{T+h}^2\right)
\end{align*}
Un prédicteur couramment utilisé est le processus linéaire. 
\begin{Def}
Un processus $\Xt$ est un processus linéaire s'il admet une décomposition de la forme suivante, $\forall t \in \Z$~:
$$
X_t = \sum_{i=-\infty}^{\infty}a_i\epsilon_{t-i}\,,
$$
où $\epsilon_t$ est un bruit blanc fort et $a_i$ est absolument sommable.
\end{Def}
L'utilisation des processus linéaire vient du Théorème de Wold, qui montre que tout processus stationnaire à l'ordre 2 peut s'écrire sous la forme d'un processus linéaire
\begin{theorem}\label{Wold}
Soit $X_t$ un processus stationnaire alors il existe une bruit blanc au sens faible $\epsilon_t$ et des coefficients réels $\left(\Psi_t\right)$ tel que~:
$$
X_t = m+\sum_{i\geq 0}\Psi_j\epsilon_j
$$
avec $m$ le moment d'ordre 1 de $X_t$
\end{theorem}
En utilisant le théorème de Wold, on sait que le processus stationnaire $X_t$ est aussi un processus linéaire. Ainsi sur un échantillon de taille $T$, on connait le meilleur prédicateur $\hat{X}_T(h)$ au sens de la plus faible erreur quadratique moyenne. On note $I_T$ la sigma algèbre engendrée par $X_1,...,X_T$ et $M_T$ le sous espace vectoriel engendré par les variables $X_1,...,X_T$ muni de la norme $L^2$. Il s'agit du prédicteur des moindres carrées~:
$$
\hat{X}_T(h)=E\left(X_{T+h}|I_T\right)=argmin_{Y\in M_T}\left(||X_{T+h}-Y||^2\right)
$$
Dans le cas des processus linéaire, l'erreur de prévision est d'espérance nulle~:
$$
E(e_{T+h}) = E(X_{T+h})-m-\sum_{i= 0}^{h-1}\Psi_iE(\epsilon_{i})=0
$$
et de variance~:
$$
V(e_{T+h}) = \sigma^2\sum_{i=0}^{h-1}\Psi_i^2=0
$$
La variance de l'erreur permet de déterminer l'intervalle de confiance. Ainsi pour un processus gaussien, on obtient un intervalle de confiance~:
$$
\hat{X}_T(h)\pm t_{1-\alpha}\sigma\sqrt{\sum_{i=0}^{h-1}\Psi_i^2}
$$
avec $t_{1-\alpha}$ le quantile d'ordre $1-\alpha$. Ces résultats sur la qualité de l'estimateur sont valable dans le cas où l'on connait les coefficient de l'estimateur ce qui en pratique n'arrive pas. Donc la qualité de la prédiction des coefficients va impacter la qualité de la prédiction et ajouter de l'incertitude.


Dans l'intevalle de confiance la variance de l'erreur est constante au cours du temps, mais ce n'est pas le cas pour des processus de type GARCH par example dans ces cas là, il est intéressant d'étudier la variance conditionnelle et l'espérance conditionnelle.
\subsection{Horizon de prévision}
Dans le cas de processus stationnaire, la loi de distribution conditionnelle converge vers la loi de distribution non conditionnelle lorsque l'horizon tend vers l'infini, i.e~: $L(X_{T+h}|I_T)\rightarrow L(X_0)$. Seule la vitesse de convergence diffère en fonction de la mémoire du processus. Plus la mémoire du processus est courte et plus la vitesse de convergence est grande et inversement.
Ainsi lorsqu'on utilise en prévision un processus ARMA, il est particulièrement recommandé que l'horizon soit de très court terme (h=1 ou 2) car ce type de processus étant à mémoire courte, au bout de quelques pas le prédicteur va être égal à la moyenne non conditionnelle de la série et l'on ne peut pas faire de prédiction à l'on terme. Si l'on désire effectuer des prévisions à long terme les processus à mémoire longue sont plus intéressants. 

\subsection*{Exemple}
Soit $X_t$ un processus AR(1) stationnaire~:
$$
X_t = \phi X_{t-1}+\epsilon_t
$$
avec $|\phi|<1$, le prédicteur d'horizon h=1 s'écrit~:
$$
\hat{X}_t(1)=E(X_{t+1}|X_t,...,X_1)=\phi X_t
$$
et donc pour $h>0$, on a~:
$$
\hat{X}_t(h)=E(X_{t+h}|X_t,...,X_1)=\phi^hX_t
$$
ainsi le prédicteur $\hat{X}_t(h)$ converge vers l'espérance de $X_t$ quand $h\rightarrow \infty$ . La vitesse de convergence est inversement proportionnelle à la valeur du paramètre $\phi$. 



Les moments empiriques convergent vers les moments théoriques. Mais malgré cette convergence, l'autocovariance empirique fournit un estimateur très pauvre de $\gamma(h)$ pour des valeurs h proche de la taille de l'échantillon. A titre indicatif, Box et Jenkins recommandent de n'utiliser ces quantités que si $T>50$ et $h<T/4$.
Lorsque l'on estime les modèles, on calcule des autocavariances empiriques et l'on doit donc s'assurer que ces autocovariances sont significativement non nulles. Pour cela on utilise le résultat suivant~:
\begin{pro}
Si $\Xt$ est un processus linéaire tel que~:
$$
X_t = \sum_{j\in \mathbb{Z}}\phi_j\epsilon_{t-j},
$$
avec $\epsilon_t$ une suite de variable i.i.d centrée tel que $\mathbb{E}\left(\epsilon_t^4\right) = \eta\mathbb{E}\left(\epsilon_t^2\right)^2<+\infty$,* $\mathbb{E}\left(\epsilon_t^2\right)=\sigma^2$ et avec $\phi_j$ une série réelle absolument convergente. Alors l'autocovariance enmpirique converge en loi vers l'autocovariance :
$$
\left(\begin{array}{c}
\overline{\gamma}_T(0)\\
.\\
.\\
.\\
\overline{\gamma}_T(p)\\
\end{array}\right)\rightarrow
 \mathcal{N}\left(
\left(\begin{array}{c}
\gamma(0)\\
.\\
.\\
.\\
\gamma(p)\\
\end{array}\right),V\right)\,,
$$
avec $V$ la matrice de variance covariance~:
$$
V=\left[\eta\gamma(h)\gamma(k)+\sum_{i\in\mathbb{Z}}\gamma(i)\gamma(i+k-h)+\gamma(i+k)\gamma(i-h)\right]_{h,k=0,...,p}
$$
\end{pro}
\section{Processus}
\subsection{processus MA}
\begin{Def}
Le processus $\Xt$ est une moyenne mobile d'ordre q s'il existe un bruit blanc $\epsilon_t\sim BB(0,\sigma^2)$ et des réels $\theta_1,...,\theta_q$ tels que~:~
$$
X_t=m+\epsilon_t+\theta_1\epsilon_{t-1}+...+\theta_q\epsilon_{t-q}
$$ 
On note MA(q) une moyenne mobile d'ordre q.
\end{Def}
Un processus MA(q) est stationnaire, la fonction d'auto-covariance est ~:
$$
\gamma(h)=\left\{\begin{array}{cc}
0 & \textrm{si |h|>q}\\
\sigma^2(1+\sum_{i=1}^q\theta^2_i) & h=0\\
\sigma^2 \theta^2_q & h=q\\
\sigma^2(\theta_h+\sum_{i=h+1}^q\theta_i\theta_{i-h}) & si 0<h<q\\
\end{array}
\right.
$$
Il en résulte que la fonction d'auto-correlation s'écrit~:
$$
\gamma(h)=\left\{\begin{array}{cc}
0 & \textrm{si |h|>q}\\
1 & h=0\\
\frac{\theta_h+\sum_{i=h+1}^q\theta_i\theta_{i-h})}{1+\theta_1^2+...+\theta_q^2} & si 0<h\leq q\\
\end{array}
\right.
$$
Il n'y a pas de résultats particulier pour les auto-corrélations partielles.
\subsubsection{processus multidimensionnel : VMA(q)}
\begin{Def}
Un processus vectoriel $\Xt$ de dimension (n,1) admet une représentation VMA d'ordre q, notée VMA(q) si~:
$$
X_t = \mu + \epsilon_t +\Psi_1\epsilon_{t-1}+\Psi_2\epsilon_{t-2}+...+ \Psi_q\epsilon_{t-q}
$$
ou de façon équivalent~:
$$
X_t = \mu+\Psi(L)\epsilon_t
$$
où $\mu$ désigne l'espérance du processus vectoriel, $\Psi(L)=\sum_i\Psi_iL^i$. Le vecteur des innovation $\epsilon_t$ est i.i.d. centrée et de matrice covariance définie positive.
\end{Def}


Le théorème de Wold (\ref{Wold}) s'applique également dans le cas multidimensionnel. En effet si un processus vectoriel est stationnaire à l'ordre 2 alors on peut le représenter sous la forme d'un processus VMA($\infty$).
\subsection{processus AR}
\begin{Def}
Un processus $\Xt$ est un processus auto-régressif d'ordre p noté AR(p) si~:
\begin{itemize}
\item[i)] $\Xt$ est stationnaire
\item[ii)] $\Xt$ vérifie~:
$$
X_t = \mu + \phi_1X_{t-1}+...+\phi_pX_{t-p}+\epsilon_t
$$
avec $\phi_p\not=0$ et $\epsilon_t\sim BB(0,\sigma^2)$
\end{itemize}
\end{Def}
On pose $\Phi(z)=1-\phi_1z-...-\phi_pz^p$, un processus AR(p) peut s'écrire à l'aide de polynôme de retard~:
$$
\Phi(L)X_t = mu + \epsilon_t
$$
A l'aide de cette formulation, on en déduit les racines du polynôme $\Phi$ sont toutes de module strictement supérieur à 1.
\begin{pro}
Soit $\Xt$ un processus AR(p), alors~:
\begin{itemize}
\item $\mathbb{E}\left(X_t\right) = \frac{\mu}{1-\phi_1-...-\phi_p}$\\
\item on pose $Y_t=X_t-\mathbb{E}\left(X_t\right)$ alors $Y_t$ est un processus AR(p) d'espérance nulle
\end{itemize}
\end{pro}
Les processus AR(p) sont des processus stationnaires, on a vu que les racines du polynômes de retard sont toutes strictement supérieure à 1, cela signifie que l'on peut inverser le polynôme de retard. Ainsi tout processus AR(p) peut s'écrire : $X_t = \Phi(L)^{-1}\left(\mu + \epsilon_t\right)$. L'inverse donnant une somme infinie, cela veut dire que tout processus AR(p) peut s'écrire sous la forme MA($\infty$).

\subsubsection{Propriétés des processus AR(p)}
On a vu précédemment que d'un processus AR(p) admettant une espérance non nulle, on pouvait toujours se ramenait à un processus AR(p) d'espérance nulle. Dans la suite on considérera le processus AR(p) $\Xt$ d'espérance nulle. L'auto covariance s'écrit pour tout h>0 : 
\begin{align*}
\gamma(h)&= Cov(X_t,X_{t-h})\\
&= \mathbb{E}\left(X_tX_{t-h}\right)\\
&= \sum_{i=1}^p\phi_i\mathbb{E}\left(X_{t-i}X_{t-h}\right)+\mathbb{E}\left(\epsilon_tX_{t-h}\right)\\
&= \sum_{i=1}^p\phi_i\gamma(h-i)
\end{align*}

Pour h=0, on a~:
\begin{align*}
\gamma(0)&= \mathbb{E}\left(X_t^2\right)\\
&= \sum_{i=1}^p\phi_i\mathbb{E}\left(X_{t-i}X_{t}\right)+\mathbb{E}\left(\epsilon_tX_{t}\right)\\
&= \sum_{i=1}^p\phi_i\gamma(i)+\mathbb{E}\left(\epsilon_t\epsilon_{t}\right)\\
&=\sum_{i=1}^p\phi_i\gamma(i)+\sigma^2
\end{align*}
On a de même pour l'auto-corrélation~:~$\rho(h)=\phi_1\rho(h-1)+...+\phi_p\rho(h-p)$ pour tout h>0.
Ces équations sont appelés equation de Yule-Walker. 
On a une relation de récurrence~:
$$
r(0)=1=\sum_{i=1}^p\phi_i\rho(i)+\frac{\sigma^2}{\gamma(0)}
$$
d'où~:
$$
\gamma(0)=\frac{\sigma^2}{1-\sum_{i=1}^p\phi_i\rho(i)}
$$
Les equations de Yule-Walker peuvent s'écrire~:
$$
\left(
\begin{array}{cccc}
1 & \rho(1) & ... & \rho(p-1)\\
\rho(1) & 1 & ... &  \rho(p-2)\\
\rho(2) & \rho(1) & ... & \rho(p-3)\\
.. & ... & ... & ...\\
\rho(p-1) & \rho(p-1) & ... & 1
\end{array}
\right)
\left(
\begin{array}{c}
\phi_1\\
. \\
. \\
.\\
\phi_p
\end{array}
\right)=\left(
\begin{array}{c}
\rho(1)\\
. \\
. \\
.\\
\rho(p)
\end{array}
\right)
$$
Les solutions de ce système sont données par les valeurs initiales $\rho(i)$. Ainsi si l'on peut estimer les corrélation sur un échantillon donnée on pourra en déduire les coefficients du processus AR(p).
\begin{pro}
Soit $\Xt$ un processus AR(p) alors~:
\begin{itemize}
\item[i)] $|\rho(h)|$ et $\gamma(h)$ décroissent exponentiellement avec h\\
\item[ii)] l'auto corrélation partielle est nulle pour h>p
\end{itemize}
\end{pro}
On peut utiliser le polynome de retard afin déterminer l'auto correlation des processus AR.
\begin{pro}
Si le polynome de retard $\Phi(L)$ définissant le processus AR(p) (i.e. $\Phi(L)X_t=m+\epsilon_t$ admet $p$ racines distinctes $(\lambda_i)^p_{i=1}$, l'autocorellation d'ordre k est déterminée par la relation~:
$$
\rho_k = A_1\left(\frac{1}{\lambda_1}\right)^k+...+A_p\left(\frac{1}{\lambda_p}\right)^k\,,
$$
où les paramètres $A_i$ sont des constantes déterminées par les conditions initiales.
\end{pro}
Il en résulte que si les racines du  de retard sont de module strictement supérieur à 1 alors l'auto corrélation tend vers zéro. Si les racines sont réels on a une exponentielle amortie et si les racines sont complexes on aura une sinusoïdale amortie.

\subsubsection{processus mutlidimensionnel : VAR(p)}
\begin{Def}
Un processus vectoriel $\Xt$ de dimension (n,1) admet une représentation VAR d'ordre p, notée VAR(p) si~:
$$
X_t = c - \Phi_1X_{t-1}-\Phi_2X_{t-2}-...- \Phi_pX_{t-p}+\epsilon_t
$$
ou de façon équivalent~:
$$
\Phi(L)X_t = c+\epsilon_t
$$
où $c$ désigne un vecteur constant de dimension (n,1), $\Phi(L)=\sum_i\Phi_iL^i$. Le vecteur des innovation $\epsilon_t$ est i.i.d. centrée et de matrice covariance définie positive.
\end{Def}
Ainsi pour un vecteur de dimension (n,1) qui suit un processu VAR(p), on aura pour tout $j$~:
\begin{align*}
x_{j,t}=&c_1+\Phi_{j,1}^1x_{1,t-1}+\Phi_{j,2}^1x_{2,t-1}+....+\Phi_{j,n}^1x_{n,t-1}\\
& \Phi_{j,1}^2x_{1,t-2}+\Phi_{j,2}^2x_{2,t-2}+....+\Phi_{j,n}^2x_{n,t-2}\\
&+...+\\
& \Phi_{j,1}^px_{1,t-p}+\Phi_{j,2}^1x_{2,t-p}+....+\Phi_{j,n}^px_{n,t-p}
\end{align*}
Comme pour les processus AR(p) un processus VAR(p) sera stationnaire si et seulement si toutes racines du déterminant polynôme de retard $\Phi(L)$ sont toutes supérieures à l'unité en module. Ce qui est équivalent au fait que toute les valeurs propres de la 'application linéaire $\Phi(L)$ sont toutes inférieur à l'unité en module. Une conséquence du théorème de Wold (\ref{Wold}) est que tout processus VAR(p) stationnaire peut s'écrire sous la forme d'un processus VMA($\infty$). On peut également écrire le polynome $\Psi$ de la moyenne mobile à l'aide du polynome $\Phi$.
\begin{pro}
Le polynôme matriciel $\Psi(L)=\sum_i \psi_iL^i$ associé à la représentation VMA($\infty$) d'un processus VAR(p) stationnaire $\Phi(L)X_t = c + \epsilon_t$, satisfait la relation de récurrence suivante~:
\begin{align*}
\psi_0&=I_n\\
\psi_s &= \Phi_1\psi_{s-1}+\Phi_2\psi_{s-2}+...+\Phi_p\psi_{s-p},\forall s\geq 1\\
\psi_s&=0, \forall s <0.
\end{align*}
\end{pro}


Un propriété intéressante pour les processus VAR(p) stationnaire est qu'ils peuvent être transformé en un processus VAR(1) d'espérance nulle.
\begin{pro}
Tout processus vectoriel stationnaire $\Xt$ satisfaisant une représentation VAR(p) peut être transformé en un processus $\tilde{X}_t$ satisfaisant une représentation VAR(1) d'espérance nulle.
\end{pro}
\textit{preuve : }
\newline
On a pour tout $t$~:~$\Phi(L)X_t = c + \epsilon_t$ avec $\Phi(L)$ une matrice carrée de taille n. Comme $X_t$ est stationnaire, on a~:
$$
\E{X_t}=\Phi(L)^{-1}\E{c+\epsilon_t}=\Phi^{-1}(L)c=\mu
$$
On a donc~:
\begin{align*}
\Phi(L)(X_t-\mu) & = \epsilon_t\\
(X_t-\mu) & = \Phi_1(X_{t-1}-\mu)+...+\Phi_p(X_{t-p}-\mu)+\epsilon_t
\end{align*}
On pose~:
$$
\tilde{X}_t = \left(
\begin{array}{c}
X_t-\mu \\
X_{t-1}-\mu \\
X_{t-2}-\mu \\
...\\
X_{t-p+1}-\mu 
\end{array}
\right)
\quad
v_t = \left(
\begin{array}{c}
\epsilon_t \\
0_{(n,1)} \\
0_{(n,1)} \\
...\\
0_{(n,1)} 
\end{array}
\right)
$$
et 
$$
A = \left(
\begin{array}{ccccc}
\Phi_1 & \Phi_2 & ... & \Phi_{p-1} & \Phi_p \\
I_n & 0_{(n,n)} & ... & 0_{(n,n)} & 0_{(n,n)} \\
0_{(n,n)}& I_n & ... & 0_{(n,n)} & 0_{(n,n)} \\
...\\
0_{(n,n)}& 0_{(n,n)} & ... &  I_n& 0_{(n,n)} 
\end{array}
\right)
$$
il en résulte que $\tilde{X}_t$ est un processus VAR(1) qui s'écrit~:~$\tilde{X}_t = A\tilde{X_{t-1}}+v_t$, avec A une matrice carrée de taille np.
\subsubsection*{Exemple}
On considère le processus bi-varié $Y_t$~:
\begin{align*}
y_{1,t}&=3+ .2y_{1,t-2}+.7y_{2,t-1}-.4y_{1,t-2}-.6y_{2,t-2}+\epsilon_{1,t}\\
y_{2,t}&=1+ .3y_{1,t-2}+.4y_{2,t-1}-.1y_{1,t-2}-.8y_{2,t-2}+\epsilon_{2,t}
\end{align*}
le processus $Y_t$ sous forme matricielle~:
$$
\Phi(L)Y_t = c +\epsilon_t =
 \left(
\begin{array}{c}
3\\
1
\end{array}
\right)+
\left(
\begin{array}{c}
\epsilon_{1,t}\\
\epsilon_{2,t}
\end{array}
\right)
$$
avec 
\begin{align*}
\Phi(L)&=\Phi_0+\Phi_1L+\phi_2L^2\\
&=\left(
\begin{array}{cc}
1 & 0\\
0 & 1
\end{array}
\right) + 
\left(
\begin{array}{cc}
-.2 & -.7\\
-.3 & -.4
\end{array}
\right)L +
\left(
\begin{array}{cc}
.4 & .6\\
.1 & .8
\end{array}
\right)L^2 \\
&= \left( \begin{array}{cc}
1-.2L+.4L^2 & -.7L+.6L^2\\
-.3L+.1L^2 & 1-.4L+.8L^2\\
\end{array}
\right)
\end{align*}
Déterminons $\E{Y_t}$~:
$$
\E{Y_t} = \Phi(1)^{-1}c=\left(
\begin{array}{c}
2.59\\
1.8
\end{array}
\right)=\mu
$$
on pose alors~:
$$
\tilde{Y}_t = \left(
\begin{array}{c}
Y_t-\mu\\
Y_{t-1}-\mu
\end{array}
\right)=
\left(
\begin{array}{c}
y_{1,t}-2.59\\
y_{2,t}-1.8\\
y_{1,t-1}-2.59\\
y_{2,t-1}-1.8\\
\end{array}
\right)\quad \textrm{et}\quad v_t=\left(\begin{array}{c}
\epsilon_{1,t}\\
\epsilon_{2,t}\\
0\\
0
\end{array}\right)
$$
et
$$
A=\left(
\begin{array}{cccc}
-.2 & -.7 & .4 & .6\\
-.3 & -.4 & .1 & .8\\
1 & 0 & 0 & 0\\
0 & 1 & 0 & 0
\end{array}
\right)
$$
on obtient alors~:~$\tilde{Y}_t=A\tilde{Y}_{t-1}+v_t$
\subsection{processus ARMA}
\begin{Def}
Un processus $X_t$ d'ordre 2 (espérance $\mu$)est un processus ARMA(p,q), si et seulement si il est stationnaire et si pour $t$ il vérifie~:
$$
\phi(L)(X_t-\mu) = \theta(L)\epsilon_t
$$
avec $\epsilon_t$ est un bruit blanc de variance $\sigma^2$, $L$ le polynome de retard et~:
\begin{align*}
\phi(z)&=1-\phi_1z-...-\phi_pz^p\\
\theta(z)&=1+\theta_1z+...+\theta_qz^q
\end{align*}
\end{Def}
La fonction d'autocovariance des processus ARMA(p,q) suit la relation de récurrence~:
$$
\gamma_k=\phi_1\gamma_{k-1}+...+\phi_p\gamma_{k-p}\,,\forall k>q
$$
Il s'agit de la même relation de récurrence qu'un processus AR(p) (équations de Yule-Walker). On n'a pas la relation pour $k\leq q$. On obtient la même relation pour l'autocorellation. L'auto-corellation partielle d'un processus à le même comportement que pour les processus MA. L'écriture des processus ARMA va dépendre des racines des polynomes de retard correspondant au composant AR et MA
\begin{pro}
Soit $X_t$ un processus ARMA(p,q)~:~$\phi(L)X_t=\mu+\theta(L)\epsilon_t$, alors~:
\begin{itemize}
\item[i)] Si le polynôme $\phi$ ne s'annule pas sur le cercle unité alors $X_t$ est un processus linéaire
\item[ii)]Si le polynôme $\phi$ ne s'annule pas sur le disque unité alors $X_t$ admet une représentation causale
\item[iii)]Si le polynome $\theta$ ne s'annule pas sur le disque unité alors $X_t$ admet une représentation inversible
\end{itemize}
\end{pro}
En résumé~:
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
Modèles & MA(q) & AR(p) & ARMA(p,q)\\
\hline
auto-correlation & $\rho(h)=0, \forall h>q$ &  $\rho(h)\rightarrow 0$ & $\rho(h)\rightarrow 0$ \\
\hline
auto-correlation partielle & $r(h)\rightarrow 0$ & $r(h)=0, \forall h>p$ &$r(h)\rightarrow 0$  \\
\hline
\end{tabular}
\end{center}
\subsection{SARMA modèles}
\subsection{ARIMA modèles}
\subsection{SARIMA modèles}
\subsection{ARCH modèles}
Un des problèmes des modèles stationnaires est que la volatilité est constante dans le temps, pour certain types de données cela n'est pas le cas. Par exemple pour des données financières on observe des clusters de volatilité, i.e. une hausse est souvent suivi d'une hausse encore plus importante. Cela signifie que la série temporelle est corrélé en temps. Campbell, Lo and Mackinglay \cite{NonLinDef} ont défini différentes notions de non linéarité~:
\begin{Def}\label{nonlin}
\begin{itemize}
\item[i)] série temporelle linéaire~:~les chocs sont non corrélés mais nécessairement indépendant et identiquement distribué.\\
\item[ii)] série temporelle non linéaire~:~ les chocs sont supposés i.i.d, mais il y a des fonctions non linéaire associées à la série temporelle et aux chocs.
$$
X_t= g(\epsilon_{t-1},\epsilon_{t-2},...)+\epsilon_th(\epsilon_{t-1},\epsilon_{t-2},...)
$$
ainsi l'espérance de $X_t$ est donnée par g et la variance conditionnelle est donnée par $h^2$
\end{itemize}
\end{Def}
Ainsi on controle la variance conditionnelle via la fonction $h$. Les processus ARCH sont des processus non linéaires en variance conditionelle, d'espérance nulle et de variance constante. Ce type de comportement n'est pas possible pour des processus stationnaire. En effet d'après le théorème de Wold (\ref{Wold}) un processus stationnaire d'espérance nulle va s'écrire comme la somme infinie de bruit blanc~:
$$
X_t = \sum_{i\geq 0}\psi_i\epsilon_{t-i}
$$
avec $\psi_0=1$. Il en résulte que la variance est constante et que l'espérance conditionnelle s'écrit~: 
$$
\E{X_t|\epsilon_{t-1},\epsilon_{t-2},...}=\sum_{i\geq 1}\psi_i \epsilon_{t-i}
$$
Ainsi la variance conditionnelle d'un processus stationnaire est~:
\begin{align*}
\mathbb{V}\left(X_t|\epsilon_{t-1},\epsilon_{t-2},...\right)&= \E{\left(X_t-\E{X_t|\epsilon_{t-1},\epsilon_{t-2},...}\right)^2} \\
&= \E{\left(X_t-\E{X_t|\epsilon_{t-1},\epsilon_{t-2},...}\right)^2|\epsilon_{t-1},\epsilon_{t-2},...}\\
&= \E{\epsilon_t^2}\\
&= \sigma^2
\end{align*}
On obtient une variance conditionnelle qui est constante ce qui ne permet pas de modéliser les variances conditionnelles non linéaires. Afin de pouvoir le faire Engle \cite{archEngle} a défini les processus ARCH
\begin{Def}\label{archDef}
Un processus $z_t$ est un processus de ARCH(q), si l'on peut l'écrire sous la forme~:
\begin{align*}
z_t &= \epsilon_t\sigma_t\\
\sigma_t^2 &= \omega + \sum_{i=1}^q \alpha_i \epsilon_{t-i}^2\,,
\end{align*}
avec $\epsilon_t$ un bruit blanc de variance $\sigma^2$ et $\omega,\alpha_1,\alpha_2,...,\alpha_q\geq 0$. 
\end{Def}
On peut vérifier que ce modèle à les propriétés souhaitées~:
\begin{align*}
\E{z_t}&=\E{\epsilon_t\sigma_t}
\end{align*}

\subsection{GARCH modèles}

\section{Annexe}
\subsection{Convergence}
Convergence $\mathcal{L}^2$~:
$$
X_n\rightarrow_{\mathcal{L^2}} X \Leftrightarrow  \lim_{n\rightarrow \infty}||X_n-X||_2=0
$$
On dit que $X_t$ convergent en loi vers $X$ si et seulement si pour toute fonction bornée $\phi$, on a~:~
$$
\lim_{n}\mathcal{E}(\phi(X_n))=\mathcal{E}\left(\phi(X)\right)
$$
\subsection{Densité spectrale}
\begin{pro}
Soit $(X_t)$ un processus stationnaire de la forme~:
$$
X_t = m+\sum_{j\geq 0}a_j\epsilon_{t-j}
$$
avec $\epsilon_t$ un bruit blanc $BB(0,\sigma^2)$ et $\sum_{j\geq 0}|a_j|<+\infty$. Alors~:
\begin{itemize}
\item $\sum_{h\in\mathbb{Z}}|\gamma(h)|<+\infty$\\
\item $\forall \omega\in[-\pi,\pi],~f_X(\omega)=\frac{1}{2\pi}\sum_{h\in\mathbb{Z}}\gamma(h)e^{i\omega h}$ est la densité spectrale de $X_t$.
\end{itemize}
\end{pro}
Sous les hypothèses précédentes, on a~:
$$
f_X(\omega)=\frac{1}{2\pi}\sum_{h\in\mathbb{Z}}\gamma(h)cos(\omega h)
$$
\subsubsection*{Exemple}
Par exemple pour un bruit blanc fort (\ref{BB}), on a~:
$$
\left\{
\begin{array}{cc}
\gamma(0)=\sigma^2&\\
\gamma(h)= 0,& \textrm{pour $h\not=0$} 
\end{array}
\right.
$$
et donc la densité spectrale du bruit blanc fort est~:
$$
f_X(\omega) = \frac{\sigma^2}{2\pi}
$$





La densité spectrale peut être utilisée pour déterminer un processus est un bruit blanc.
\begin{pro}
Si la densité spectrale d'une série $\Xt$ est constante alors ce processus est un bruit blanc fort.
\end{pro}
En effet~:
$$
\gamma_X(h)=\int_{-\pi}^{\pi}f_X(\omega)e^{i\omega h}d\omega = K\int_{-\pi}^{\pi}e^{i\omega h)d\omega}
$$
et donc si $h=0$ alors $\gamma_X(h)=2\pi K$ et si $h\not=0$ alors $\gamma_X(h)=0$. Ceci caractérise un bruit blanc. La densité spectrale peut être utilisée pour décrire les dépendances entre les processus.
\begin{pro}
Si $\Xt$ est une moyenne mobile~:
$$
X_t = \sum_{k\in\mathbb{Z}}a_k\epsilon_{t-k}\,,
$$
où $\epsilon_t$ est un bruit blanc fort de variance $\sigma^2$ et tel que $\sum_{k\in\mathbb{Z}}|a_k| <+\infty$. Si on considère $Y_t = \sum_{j\in\mathbb{Z}}\beta_jX_{t-j}$, alors on a la relation suivante~:
$$
f_Y(\omega)=f_X(\omega)\left|\sum_{j\in\mathbb{Z}}\beta_je^{i\omega j}\right|^2
$$
\end{pro}
Par exemple si $Y_t=X_t-\phi X_{t-1}$ on a~:$f_Y(\omega)=f_X(\omega)\left|1+\phi e^{i\omega}\right|^2.$
\subsection{Estimateur empirique des moments}
Soit $(X_t)_{t\in\mathbb{Z}}$ un processus stationnaire, on cherche à estimer l'espérance, les fonction d'auto-covariance, d'auto-corrélation et d'auto-corrélation partielle. On a un échantillon de taille T. On prend comme estimateur~:
\begin{itemize}
\item moyenne empirique~:~$\hat{m}=\frac{1}{T}\sum_{t=1}^TX_t=\overline{X}_T$\\
\item auto-covariance empirique d'ordre h~:~$\hat{\gamma}(h)=\frac{1}{T-h}\sum_{t=h+1}^T(X_t-\overline{X}_T)(X_{t-h}-\overline{X}_T)$\\
\item auto-corrélation empirique d'ordre h~:~$\hat{\rho}(h)=\frac{\hat{\gamma}(h)}{\hat{\gamma}(0)}$.\\
\item l'auto-corrélation partielle est une régression empirique sur 1,$X_1$,...,$X-{t-h}$.
\end{itemize}
Pour estimer la fonction de densité spectrale on utilise la auto-covariance empirique et un terme d'ajustement (coefficient de Newey-West) afin diminuer le poids de $\hat{\gamma}(h)$ pour des h grand.


Si $X_t$ est un processus stationnaire alors d'après la loi des grands nombres tous les estimateurs empiriques présentés ci-dessus convergent.

\begin{pro}
Si $X_t=m+\sum_{j\geq 0}a_j\epsilon_{t-j}$ avec $\epsilon_t$ un bruit blanc de moment d'ordre 4 fini, alors tous ses estimateurs ont des lois jointes asymptotiquement gaussiennes~:
$$
\sqrt{T}\left(\hat{m}-m\right)\rightarrow \mathcal{N}(0,\sum_{h\in\mathcal{Z}}\gamma(h)
$$
convergence en loi
\end{pro}
\subsection{polynôme de retard}
\begin{Def}
L'opérateur de retard est défini sur la classe des processus stationnaires comme étant~: 
$$
L(X_t)=X_{t-1}
$$
\end{Def}
\begin{Def}
Soit $P$ un polynôme, $P(z)=\sum_{k=0}^{p}a_kz^k$ avec $a_k\in\mathbb{R}$, on lui associe le polynôme de retard $P(L)$ défini comme suit~:
$$
P(L)=\sum_{k=0}^pa_kL^k
$$
et 
$$
P(L)X_t=\sum_{k=0}^pa_kX_{t-k}
$$
\end{Def}
Un polynôme de retard P(L) est inversible s'il existe un polynôme de retard $Q(L)$ tel que $P(L)Q(L)=Id$. Dans $\mathbb{C}$, tout polynôme réel $P$ se décompose~:
$$
P(z)=\prod_{i=1}^p(z-z_i)=\prod_{i=1}^p(-z_i)\prod_{i=1}^p(1-\frac{z}{z_i})=\alpha\prod_{i=1}^p(1-\lambda_i z)
$$
avec $\lambda_i=\frac{1}{z_i}$.
Le polynôme est $(1-\lambda z)$ est inversible si seulement si ses racines sont de modules strictement inférieur à 1. Ceci équivalent à $|\lambda|>1$. On peut généraliser ce résultat à tout polynôme de degré p.
\begin{pro}
Soit $P(L) = \alpha\prod_{i=1}^p(1-\lambda_i L)$ un polynôme de retard, si pour tout i $|\lambda_i|<1$ alors $P(L)$ est inversible et :
$$
P^{-1}(L)= \sum_{k\geq 0}b_kL^k
$$
\end{pro}

\subsection{Chaine de Markov}
\begin{Def}
Le processus $\left\{X_t\right\}_{t\in\mathbb{N}}$ est une chaîne de Markov d'ordre p si et seulement si, pour tout $t$, on a l'égalité de distributions~:
$$
\mathcal{L}\left(X_t|X_{t-1},X_{t-2},X_{t-3},....\right) = \mathcal{L}\left(X_t|X_{t-1},...,X_{t-p}\right)
$$
\end{Def}
\begin{theorem}
Le processus $\left\{\right\}_{t\in\mathbb{N}}$ est une chaine de markov d'ordre 1 si et seulement s'il existe une fonction $g$ mesurable et un processus $\epsilon_t$ tel que $X_t=g(X_{t-1},\epsilon_t)$ avec $\epsilon_t$ une suite de variable aléatoire indépendante et de même loi.
\end{theorem}
Un processus AR(1) est un processus markovien, de même toute marche aléatoire une porcessus markovien d'ordre 1.

\bibliographystyle{amsplain}
%\bibliography{C:/Users/jerpetit/Desktop/master/TRIED/memoireM2/biblio}
\bibliography{C:/Users/jerom/PycharmProjects/memoireM2/biblio}
\end{document}
