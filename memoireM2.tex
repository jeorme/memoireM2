%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Wenneker Assignment
% LaTeX Template
% Version 2.0 (12/1/2019)
%
% This template originates from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Vel (vel@LaTeXTemplates.com)
% Frits Wenneker
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt]{scrartcl} % Font size
\usepackage{hyperref} % for url

\input{structure.tex} % Include the file specifying the document structure and custom commands
\newtheorem{theorem}{Théorème}[section]
\newtheorem{corollary}{Corollaire}[theorem]
\newtheorem{lemma}[theorem]{Lemme}
\newtheorem{Def}[theorem]{Definition}
\newtheorem{pro}[theorem]{Proposition}

%%newcommand
\newcommand{\Xt}{\left(X_t\right)_{t\in\mathbb{Z}}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\var}[1]{\mathbb{V}\left(#1\right)}
\newcommand{\E}[1]{\mathbb{E}\left(#1\right)}
\newcommand{\bb}[1]{\mathcal{BB}\left(0,#1\right)}
\newcommand{\bbn}[1]{\mathcal{BBN}\left(0,#1\right)}
%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{	
	\normalfont\normalsize
	\textsc{CNAM}\\ % Your university, school and/or department name(s)
	\vspace{25pt} % Whitespace
	\rule{\linewidth}{0.5pt}\\ % Thin top horizontal rule
	\vspace{20pt} % Whitespace
	{\huge Series temporelles : ARIMA, LSTM?}\\ % The assignment title
	\vspace{12pt} % Whitespace
	\rule{\linewidth}{2pt}\\ % Thick bottom horizontal rule
	\vspace{12pt} % Whitespace
}

\author{\LARGE Jérôme Petit} % Your name

\date{\normalsize\today} % Today's date (\today) or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	FIGURE EXAMPLE
%----------------------------------------------------------------------------------------
%
\section{Introduction}


\section{Séries temporelles}
Une série temporelle ou série chronologique est une suite d’observations d’un phénomène faites au cours du temps. Cette notion s'étend à de nombreux domaines, tout relevé de données, de flux ou d'information est une série temporelle : indice boursier, population française, trafic Internet, etc. Ses données réelles obtenues sont des suites finies indexés par un temps continu ou discret. Les modèles mathématiques utilisées pour modélisées ses séries temporelles sont eux de dimension infinie.
\subsection{Stationnarité}
\begin{Def}
Stationnarité stricte ou forte~:~$\left(X_t\right)_{t\in\mathbb{Z}}$ est un processus stationnaire au sens strict si~:~$\forall n\in \mathbb{N}, \forall (t_1,...,t_n), \forall h\in\mathbb{Z}$ la loi de $(X_{t_1},...,X_{t_n})$ est identique à la loi de $(X_{t_1+h},...,X_{t_n+h})$
\end{Def}
D'après le théorème de Kolmogorov, on en déduit qu'un processus $\left(X_t\right)$ est stationnaire au sens fort si et seulement si la loi de $X_t$ est identique à la loi de $X_{t+h}$ quelque soit $h$. Cette condition de stationnarité est très contraignante et relativement peu observée en pratique. On remarque pour un processus stationnaire strict $X_t$ admettant des moments d'ordre 1 et 2 finis, on a~:
$$
\mathbb{E}\left(X_t\right)=\mathbb{E}\left(X_{t+h}\right), \forall h\in \mathbb{N}
$$
on pose~:~$\mathbb{E}\left(X_t\right)=\mathbb{E}\left(X_0\right)=\mu$. Pour la covariance, on a~:
\begin{align*}
Cov\left(X_t,X_{t+h}\right)& = \mathbb{E}\left((X_t-\mu)(X_{t+h}-\mu)\right)\\
&= \mathbb{E}\left((X_0-\mu)(X_{h_0}-\mu)\right)
\end{align*}
la dernière égalité vient du fait que $(X_t,X_{t+h})$ a la même distribution que $(X_0,X_h)$.
On remarque donc que les processus stationnaire strict ont des moments d'ordre 1 et 2 indépendant du temps $t$. C'est cette notion qui va être utilisée pour des définir les processus stationnaires au sens faible.
\begin{Def}
Stationnarité faible~:~
$\left(X_t\right)_{t\in\mathbb{Z}}$ est un processus stationnaire du second ordre (ou processus faiblement stationnaire ) s'il vérifie~:
\begin{itemize}
\item[i)]$\forall t\in\mathbb{Z},~\mathbb{E}\left(X_t\right)=m $ 
\item[ii)]$\forall t\in\mathbb{Z},~\mathbb{V}\left(X_t\right)=\sigma^2 $ 
\item[iii)]$\forall t,h\in\mathbb{Z},~Cov\left(X_t,X_{t+h}\right)=Cov\left(X_0,X_h\right)=\gamma\left(h\right) $ 
\end{itemize}
On note $\gamma(h)$ l'autocovariance d'ordre $h$ de $X_t$ .
\end{Def} 
Dans la suite les processus stationnaires désigneront les processus stationnaire du second ordre. La fonction d'autocovariance est paire et $\gamma(0)=\var{X_t}$.
\begin{Def}
Le coefficient d'autocorrélation d'ordre $h$ d'un processus stationnaire $\Xt$ est~:
$$\rho(h)=\frac{\gamma(h)}{\gamma(0)}.$$
\end{Def}
La fonction $h\mapsto\rho(h)$ est appelée fonction d'autocorrelation.
Parmi les processus stationnaires, il y a les processus appelés bruit blanc.
\begin{Def}
Un bruit blanc $\epsilon_t$ est une suite de variable aléatoire non corrélées, mais pas nécessairement indépendantes de moyenne nulle et de variance constante $\sigma^2$. On note $\epsilon_t\sim \bb{\sigma^2}$. 
\end{Def}
Un bruit blanc est donc un processus stationnaire d'ordre 2 tel que~:
\begin{itemize}
\item $\E{X_t}=0$\\
\item $\gamma(h)=0$ si $h\not =0$
\end{itemize}
Un bruit blanc n'est pas nécessairement fortement stationnaire car les variables aléatoires ne sont pas nécessairement indépendantes.
\begin{Def}
Un bruit blanc gaussien $\epsilon_t$ est une suite de variable aléatoire i.i.d de loi normale $\mathcal{N}\left(0,\sigma^2\right)$. On note $\epsilon\sim \bbn{\sigma^2}$
\end{Def}
Un bruit blanc gaussien est un processus stationnaire au sens strict. 


Les processus non stationnaires sont des processus dont les moments d'ordre 1 ou 2 ne sont pas un constant. La marche aléatoire est un exemple de processus non stationnaire car la variance est fonction du temps.

Dans les cas pratiques, on aura une série observée pour $t=1,...,T$, dans ce cas là on devra calculer l'autocavariance (resp. autocorrelation) empirique~:

\begin{align*}
\overline{\gamma}(h)&=\frac{1}{T-h}\sum_{t=h+1}^T(y_t-\overline{y})(y_{t-h}-\overline{y}), 0\leq h\leq T-1\\
\overline{\rho}(h)&=\frac{T}{T-h}\frac{\sum_{t=h+1}^T(y_t-\overline{y})(y_{t-h}-\overline{y})}{\sum_{t=1}^T(y_t-\overline{y})^2}, 0\leq h\leq T-1,\\
\overline{y} &= \frac{1}{T}\sum_{t=1}^Ty_t
\end{align*}
Les moments empiriques convergent vers les moments théoriques. Mais malgré cette convergence, l'autocovariance empirique fournit un estimateur très pauvre de $\gamma(h)$ pour des valeurs h proche de la taille de l'échantillon. A titre indicatif, Box et Jenkins recommandent de n'utiliser ces quantités que si $T>50$ et $h<T/4$.
Lorsque l'on estime les modèles, on calcule des autocavariances empiriques et l'on doit donc s'assurer que ces autocovariances sont significativement non nulles. Pour cela on utilise le résultat suivant~:
\begin{pro}
Si $\Xt$ est un processus linéaire tel que~:
$$
X_t = \sum_{j\in \mathbb{Z}}\phi_j\epsilon_{t-j},
$$
avec $\epsilon_t$ une suite de variable i.i.d centrée tel que $\mathbb{E}\left(\epsilon_t^4\right) = \eta\mathbb{E}\left(\epsilon_t^2\right)^2<+\infty$,* $\mathbb{E}\left(\epsilon_t^2\right)=\sigma^2$ et avec $\phi_j$ une série réelle absolument convergente. Alors l'autocovariance enmpirique converge en loi vers l'autocovariance :
$$
\left(\begin{array}{c}
\overline{\gamma}_T(0)\\
.\\
.\\
.\\
\overline{\gamma}_T(p)\\
\end{array}\right)\rightarrow
 \mathcal{N}\left(
\left(\begin{array}{c}
\gamma(0)\\
.\\
.\\
.\\
\gamma(p)\\
\end{array}\right),V\right)\,,
$$
avec $V$ la matrice de variance covariance~:
$$
V=\left[\eta\gamma(h)\gamma(k)+\sum_{i\in\mathbb{Z}}\gamma(i)\gamma(i+k-h)+\gamma(i+k)\gamma(i-h)\right]_{h,k=0,...,p}
$$
\end{pro}
\subsection{Outil pour l'étude des processus stationnaires}
\begin{theorem}
Soit $X_t$ un processus stationnaire alors il existe une bruit blanc au sens faible $\epsilon_t$ et des coefficients réels $\left(\Psi_t\right)$ tel que~:
$$
X_t = m+\sum_{i\geq 0}\Psi_j\epsilon_j
$$
avec le moment d'ordre 1 de $X_t$
\end{theorem}
Cette décomposition est appelé décomposition de Wold ou également transformée par moyenne mobile.
Les processus stationnaires sont stable par combinaison sous certaine condition sur les coefficients : 
\begin{pro}
Si $\Xt$ est un processus stationnaire et $(a_i)_{i\in \mathbb{Z}}$ est une suite de réels absolument convergente alors le processus : 
$$
Y_t=\sum_{i\in\mathbb{Z}}a_i X_{t-i}
$$
est un processus stationnaire
\end{pro}
Une conséquence de cette proposition est que la différence ou la moyenne de processus stationnaire est un processus stationnaire.


Afin d'étudier les processus stationnaires, on utilisera les coefficients d'auto-corrélation et les coefficients d'auto corrélation partielles.
\begin{Def}
On appelle auto-corrélation partielle d'ordre p~:
$$
r(p)=\frac{Cov\left(X_t-\mathbb{E}\left(X_t|X_{t-1},...,X_{t-p}\right),X_{t-p}-\mathbb{E}\left(X_{t-p}|X_{t-1},...,X_{t-p+1}\right)\right)}{\mathbb{V}\left(X_t|X_{t-1},...,X_{t-p}\right)\mathbb{V}\left(X_{t-p}|X_{t-1},...,X_{t-p+1}\right)}
$$
\end{Def}
\subsubsection{Densité spectrale}
Les coefficients d'autocovariance d'une série stationnaire correspondent aux coefficients de Fourier d'une mesure positive, appelée mesure spectrale du processus. Il est possible de montrer que cette mesure spectrale admet un densité, dite spectrale, par rapport à la mesure de Lebesgues sur $[-\pi,\pi]$ que nous noterons $f_X$.
\begin{Def}
Soit $\Xt$ un processus stationnaire de fonction d'autocovariance $\gamma_X$ la densité spectrale de $\Xt$ s'écrit~:
$$
f_X(\omega) = \frac{1}{2\pi}\sum_{h\in\mathbb{Z}}\gamma_X(h)e^{i\omega h}\,.
$$
\end{Def}
\begin{pro}
Si $f_X$ est la densité spectrale de $\Xt$ alors~:
$$
\gamma_X(h)=\int_{-\pi}^{\pi}f_X(\omega)e^{i\omega h}d\omega
$$
\end{pro}
Par exemple pour un bruit blanc fort, on a~:
$$
\left\{
\begin{array}{cc}
\gamma(0)=\sigma^2&\\
\gamma(h)= 0,& \textrm{pour $h\not=0$} 
\end{array}
\right.
$$
et donc la densité spectrale du bruit blanc fort est~:
$$
f_X(\omega) = \frac{\sigma^2}{2\pi}
$$
La densité spectrale peut être utilisée pour déterminer un processus est un bruit blanc.
\begin{pro}
Si la densité spectrale d'une série $\Xt$ est constante alors ce processus est un bruit blanc fort.
\end{pro}
En effet~:
$$
\gamma_X(h)=\int_{-\pi}^{\pi}f_X(\omega)e^{i\omega h}d\omega = K\int_{-\pi}^{\pi}e^{i\omega h)d\omega}
$$
et donc si $h=0$ alors $\gamma_X(h)=2\pi K$ et si $h\not=0$ alors $\gamma_X(h)=0$. Ceci charactérise un bruit blanc. La densité spectrale peut être utilisée pour décrire les dépendances entre les processus.
\begin{pro}
Si $\Xt$ est une moyenne mobile~:
$$
X_t = \sum_{k\in\mathbb{Z}}a_k\epsilon_{t-k}\,,
$$
où $\epsilon_t$ est un bruit blanc fort de variance $\sigma^2$ et tel que $\sum_{k\in\mathbb{Z}}|a_k| <+\infty$. Si on considère $Y_t = \sum_{j\in\mathbb{Z}}\beta_jX_{t-j}$, alors on a la relation suivante~:
$$
f_Y(\omega)=f_X(\omega)\left|\sum_{j\in\mathbb{Z}}\beta_je^{i\omega j}\right|^2
$$
\end{pro}
Par exemple si $Y_t=X_t-\phi X_{t-1}$ on a~:$f_Y(\omega)=f_X(\omega)\left|1+\phi e^{i\omega}\right|^2.$
\subsection{Non stationnarité}
La stationnarité d'ordre 2 se définie comme une invariance des moments d'ordre 1 et 2. Par opposition, on dira qu'une série est non-stationnaire si elle n'est pas stationnaire. La classe des processus non stationnaire est plus vaste que celle des processus stationnaire. Parmi les processus non stationnaire Nelson et Plosser \cite{NonSta} ont retenu deux classes de processus non-stationnaires : les processus TS \textit{(trend stationary)} et les processus DS \textit{(difference stationary)}. Les premiers correspondent à une non-stationnarité de type déterministe alors que les seconds correspondent à une non-stationnarité de type stochastique.
\begin{Def}
$\Xt$ est un processus non-stationnaire TS s'il peut s'écrire sous la forme $X_t = f(t)+Z_t$ où $f(t)$ est une fonction déterministe du temps et $Z_t$ est un processus stationnaire.
\end{Def}
L'exemple le plus simple est celui de la tendance linéaire bruitée~:~$X_t=a+bt+\epsilon_t$. Ce processus admet une tendance déterministe $a+bt$. Les processus non-stationnaire TS non pas de persistance des chocs. L'influence d'un choc subit à un instant $\tau$ aura tendance ) s'estomper au cours du temps et la variable rejoint alors sa dynamique à long terme déterminé par $f(t)$. Cette propriété une conséquence directe de la décomposition de Wold.
\begin{Def}
Un processus $\Xt$ est un processus non-stationnaire DS ou intégré d'ordre $d$ si le processus obtenu après $d$ différenciation est stationnaire.
\end{Def}
Le fait de différencer $d$ fois revient à multiplier par le polynôme de retard $(1-L)^d$ et cela revient donc à chercher les racines de l'unité. Car si le polynome $\Phi(L)X_t$ est stationnaire et si 1 est une racine du polynôme $\Phi$ alors $\left(X_t\right)$ sera non stationnaire. C'est pour cela que les tests de détections de racine de l'unité sont utilisés pour la détection de non stationnarité.
\subsection{Processus à mémoire}
L'auto correlation est aussi utilisé pour décrire la mémoire des processus.
\begin{Def}
Un processus est dit à mémoire courte s'il possède une ACF, $\rho(k)$, telle que~:
$$
\rho(k)\leq Cr^{-k}
$$
avec $C>0$ et $0<r<1$
\end{Def}
Par exemple, les modèles MA(1) et AR(1) sont des processus à mémoire courte.
\begin{Def}
Un processus est dit à mémoire longue s'il possède une ACF, $\rho(k)$, qui est approchée comme suit~:
$$
\rho(k)\sim_{k\rightarrow \infty} Ck^{-\alpha}
$$
avec $C>0$ une constante et $\alpha\in ]0,1[$.
\end{Def}
Pour des processus à mémoire longue la série des autocorellations est absolument divergente. Les processus fractionnaires de type ARFIMA permettent de reproduire ce comportement.
\subsection{Prédiction de séries temporelles}
La prédiction des séries temporelles consiste à 
\section{Processus}
\subsection{processus MA}
\begin{Def}
Le processus $\Xt$ est une moyenne mobile d'ordre q s'il existe un bruit blanc $\epsilon_t\sim BB(0,\sigma^2)$ et des réels $\theta_1,...,\theta_q$ tels que~:~
$$
X_t=m+\epsilon_t+\theta_1\epsilon_{t-1}+...+\theta_q\epsilon_{t-q}
$$ 
On note MA(q) une moyenne mobile d'ordre q.
\end{Def}
Un processus MA(q) est stationnaire, la fonction d'auto-covariance est ~:
$$
\gamma(h)=\left\{\begin{array}{cc}
0 & \textrm{si |h|>q}\\
\sigma^2(1+\sum_{i=1}^q\theta^2_i & h=0\\
\sigma^2 \theta^2_q & h=q\\
\sigma^2(\theta_h+\sum_{i=h+1}^q\theta_i\theta_{i-h} & si 0<h<q\\
\end{array}
\right.
$$
Il n'y a pas de résultats particulier pour les auto-corrélations partielles.
\subsection{processus AR}
\begin{Def}
Un processus $\Xt$ est un processus auto-régressif d'ordre p noté AR(p) si~:
\begin{itemize}
\item[i)] $\Xt$ est stationnaire
\item[ii)] $\Xt$ vérifie~:
$$
X_t = \mu + \phi_1X_{t-1}+...+\phi_pX_{t-p}+\epsilon_t
$$
avec $\phi_p\not=0$ et $\epsilon_t\sim BB(0,\sigma^2)$
\end{itemize}
\end{Def}
On pose $\Phi(z)=1-\phi_1z-...-\phi_pz^p$, un processus AR(p) peut s'écrire à l'aide de polynôme de retard~:
$$
\Phi(L)X_t = mu + \epsilon_t
$$
A l'aide de cette formulation, on en déduit les racines du polynôme $\Phi$ sont toutes de module strictement supérieur à 1.
\begin{pro}
Soit $\Xt$ un processus AR(p), alors~:
\begin{itemize}
\item $\mathbb{E}\left(X_t\right) = \frac{\mu}{1-\phi_1-...-\phi_p}$\\
\item on pose $Y_t=X_t-\mathbb{E}\left(X_t\right)$ alors $Y_t$ est un processus AR(p) d'espérance nulle
\end{itemize}
\end{pro}
Les processus AR(p) sont des processus stationnaires, on a vu que les racines du polynômes de retard sont toutes strictement supérieure à 1, cela signifie que l'on peut inverser le polynôme de retard. Ainsi tout processus AR(p) peut s'écrire : $X_t = \Phi(L)^{-1}\left(\mu + \epsilon_t\right)$. L'inverse donnant une somme infinie, cela veut dire que tout processus AR(p) peut s'écrire sous la forme MA($\infty$).

\subsubsection{Propriétés des processus AR(p)}
On a vu précédemment que d'un processus AR(p) admettant une espérance non nulle, on pouvait toujours se ramenait à un processus AR(p) d'espérance nulle. Dans la suite on considérera le processus AR(p) $\Xt$ d'espérance nulle. L'auto covariance s'écrit pour tout h>0 : 
\begin{align*}
\gamma(h)&= Cov(X_t,X_{t-h})\\
&= \mathbb{E}\left(X_tX_{t-h}\right)\\
&= \sum_{i=1}^p\phi_i\mathbb{E}\left(X_{t-i}X_{t-h}\right)+\mathbb{E}\left(\epsilon_tX_{t-h}\right)\\
&= \sum_{i=1}^p\phi_i\gamma(h-i)
\end{align*}

Pour h=0, on a~:
\begin{align*}
\gamma(0)&= \mathbb{E}\left(X_t^2\right)\\
&= \sum_{i=1}^p\phi_i\mathbb{E}\left(X_{t-i}X_{t}\right)+\mathbb{E}\left(\epsilon_tX_{t}\right)\\
&= \sum_{i=1}^p\phi_i\gamma(i)+\mathbb{E}\left(\epsilon_t\epsilon_{t}\right)\\
&=\sum_{i=1}^p\phi_i\gamma(i)+\sigma^2
\end{align*}
On a de même pour l'auto-corrélation~:~$\rho(h)=\phi_1\rho(h-1)+...+\phi_p\rho(h-p)$ pour tout h>0.
Ces équations sont appelés equation de Yule-Walker. 
On a une relation de récurrence~:
$$
r(0)=1=\sum_{i=1}^p\phi_i\rho(i)+\frac{\sigma^2}{\gamma(0)}
$$
d'où~:
$$
\gamma(0)=\frac{\sigma^2}{1-\sum_{i=1}^p\phi_i\rho(i)}
$$
Les equations de Yule-Walker peuvent s'écrire~:
$$
\left(
\begin{array}{cccc}
1 & \rho(1) & ... & \rho(p-1)\\
\rho(1) & 1 & ... &  \rho(p-2)\\
\rho(2) & \rho(1) & ... & \rho(p-3)\\
.. & ... & ... & ...\\
\rho(p-1) & \rho(p-1) & ... & 1
\end{array}
\right)
\left(
\begin{array}{c}
\phi_1\\
. \\
. \\
.\\
\phi_p
\end{array}
\right)=\left(
\begin{array}{c}
\rho(1)\\
. \\
. \\
.\\
\rho(p)
\end{array}
\right)
$$
Les solutions de ce système sont données par les valeurs initiales $\rho(i)$. Ainsi si l'on peut estimer les corrélation sur un échantillon donnée on pourra en déduire les coefficients du processus AR(p).
\begin{pro}
Soit $\Xt$ un processus AR(p) alors~:
\begin{itemize}
\item[i)] $|\rho(h)|$ et $\gamma(h)$ décroissent exponentiellement avec h\\
\item[ii)] l'auto corrélation partielle est nulle pour h>p
\end{itemize}
\end{pro}
\subsection{ARMA modèles}

\subsection{SARMA modèles}
\subsection{ARIMA modèles}
\subsection{SARIMA modèles}
\subsection{ARCH modèles}
\subsection{GARCH modèles}

\section{Annexe}
\subsection{Convergence}
Convergence $\mathcal{L}^2$~:
$$
X_n\rightarrow_{\mathcal{L^2}} X \Leftrightarrow  \lim_{n\rightarrow \infty}||X_n-X||_2=0
$$
On dit que $X_t$ convergent en loi vers $X$ si et seulement si pour toute fonction bornée $\phi$, on a~:~
$$
\lim_{n}\mathcal{E}(\phi(X_n))=\mathcal{E}\left(\phi(X)\right)
$$
\subsection{Densité spectrale}
\begin{pro}
Soit $(X_t)$ un processus stationnaire de la forme~:
$$
X_t = m+\sum_{j\geq 0}a_j\epsilon_{t-j}
$$
avec $\epsilon_t$ un bruit blanc $BB(0,\sigma^2)$ et $\sum_{j\geq 0}|a_j|<+\infty$. Alors~:
\begin{itemize}
\item $\sum_{h\in\mathbb{Z}}|\gamma(h)|<+\infty$\\
\item $\forall \omega\in[-\pi,\pi],~f_X(\omega)=\frac{1}{2\pi}\sum_{h\in\mathbb{Z}}\gamma(h)e^{i\omega h}$ est la densité spectrale de $X_t$.
\end{itemize}
\end{pro}
Sous les hypothèses précédentes, on a~:
$$
f_X(\omega)=\frac{1}{2\pi}\sum_{h\in\mathbb{Z}}\gamma(h)cos(\omega h)
$$
\subsection{Estimateur empirique des moments}
Soit $(X_t)_{t\in\mathbb{Z}}$ un processus stationnaire, on cherche à estimer l'espérance, les fonction d'auto-covariance, d'auto-corrélation et d'auto-corrélation partielle. On a un échantillon de taille T. On prend comme estimateur~:
\begin{itemize}
\item moyenne empirique~:~$\hat{m}=\frac{1}{T}\sum_{t=1}^TX_t=\overline{X}_T$\\
\item auto-covariance empirique d'ordre h~:~$\hat{\gamma}(h)=\frac{1}{T-h}\sum_{t=h+1}^T(X_t-\overline{X}_T)(X_{t-h}-\overline{X}_T)$\\
\item auto-corrélation empirique d'ordre h~:~$\hat{\rho}(h)=\frac{\hat{\gamma}(h)}{\hat{\gamma}(0)}$.\\
\item l'auto-corrélation partielle est une régression empirique sur 1,$X_1$,...,$X-{t-h}$.
\end{itemize}
Pour estimer la fonction de densité spectrale on utilise la auto-covariance empirique et un terme d'ajustement (coefficient de Newey-West) afin diminuer le poids de $\hat{\gamma}(h)$ pour des h grand.


Si $X_t$ est un processus stationnaire alors d'après la loi des grands nombres tous les estimateurs empiriques présentés ci-dessus convergent.

\begin{pro}
Si $X_t=m+\sum_{j\geq 0}a_j\epsilon_{t-j}$ avec $\epsilon_t$ un bruit blanc de moment d'ordre 4 fini, alors tous ses estimateurs ont des lois jointes asymptotiquement gaussiennes~:
$$
\sqrt{T}\left(\hat{m}-m\right)\rightarrow \mathcal{N}(0,\sum_{h\in\mathcal{Z}}\gamma(h)
$$
convergence en loi
\end{pro}
\subsection{polynôme de retard}
\begin{Def}
L'opérateur de retard est défini sur la classe des processus stationnaires comme étant~: 
$$
L(X_t)=X_{t-1}
$$
\end{Def}
\begin{Def}
Soit $P$ un polynôme, $P(z)=\sum_{k=0}^{p}a_kz^k$ avec $a_k\in\mathbb{R}$, on lui associe le polynôme de retard $P(L)$ défini comme suit~:
$$
P(L)=\sum_{k=0}^pa_kL^k
$$
et 
$$
P(L)X_t=\sum_{k=0}^pa_kX_{t-k}
$$
\end{Def}
Un polynôme de retard P(L) est inversible s'il existe un polynôme de retard $Q(L)$ tel que $P(L)Q(L)=Id$. Dans $\mathbb{C}$, tout polynôme réel $P$ se décompose~:
$$
P(z)=\prod_{i=1}^p(z-z_i)=\prod_{i=1}^p(-z_i)\prod_{i=1}^p(1-\frac{z}{z_i})=\alpha\prod_{i=1}^p(1-\lambda_i z)
$$
avec $\lambda_i=\frac{1}{z_i}$.
Le polynôme est $(1-\lambda z)$ est inversible si seulement si ses racines sont de modules strictement inférieur à 1. Ceci équivalent à $|\lambda|>1$. On peut généraliser ce résultat à tout polynôme de degré p.
\begin{pro}
Soit $P(L) = \alpha\prod_{i=1}^p(1-\lambda_i L)$ un polynôme de retard, si pour tout i $|\lambda_i|<1$ alors $P(L)$ est inversible et :
$$
P^{-1}(L)= \sum_{k\geq 0}b_kL^k
$$
\end{pro}

\subsection{Chaine de Markov}
\begin{Def}
Le processus $\left\{X_t\right\}_{t\in\mathbb{N}}$ est une chaîne de Markov d'ordre p si et seulement si, pour tout $t$, on a l'égalité de distributions~:
$$
\mathcal{L}\left(X_t|X_{t-1},X_{t-2},X_{t-3},....\right) = \mathcal{L}\left(X_t|X_{t-1},...,X_{t-p}\right)
$$
\end{Def}
\begin{theorem}
Le processus $\left\{\right\}_{t\in\mathbb{N}}$ est une chaine de markov d'ordre 1 si et seulement s'il existe une fonction $g$ mesurable et un processus $\epsilon_t$ tel que $X_t=g(X_{t-1},\epsilon_t)$ avec $\epsilon_t$ une suite de variable aléatoire indépendante et de même loi.
\end{theorem}
Un processus AR(1) est un processus markovien, de même toute marche aléatoire une porcessus markovien d'ordre 1.

\bibliographystyle{amsplain}
\bibliography{C:/Users/jerpetit/Desktop/master/TRIED/memoireM2/biblio}

\end{document}
