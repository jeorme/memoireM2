%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Wenneker Assignment
% LaTeX Template
% Version 2.0 (12/1/2019)
%
% This template originates from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Vel (vel@LaTeXTemplates.com)
% Frits Wenneker
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[11pt]{scrartcl} % Font size
\usepackage{hyperref} % for url

\input{structure.tex} % Include the file specifying the document structure and custom commands
\newtheorem{theorem}{Théorème}[section]
\newtheorem{corollary}{Corollaire}[theorem]
\newtheorem{lemma}[theorem]{Lemme}
\newtheorem{Def}[theorem]{Definition}
\newtheorem{pro}[theorem]{Proposition}

%%newcommand
\newcommand{\Xt}{\left(X_t\right)_{t\in\mathbb{Z}}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\var}[1]{\mathbb{V}\left(#1\right)}
\newcommand{\E}[1]{\mathbb{E}\left(#1\right)}
\newcommand{\bb}[1]{\mathcal{BB}\left(0,#1\right)}
\newcommand{\bbn}[1]{\mathcal{BBN}\left(0,#1\right)}
%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{	
	\normalfont\normalsize
	\textsc{CNAM}\\ % Your university, school and/or department name(s)
	\vspace{25pt} % Whitespace
	\rule{\linewidth}{0.5pt}\\ % Thin top horizontal rule
	\vspace{20pt} % Whitespace
	{\huge Series temporelles : ARIMA, LSTM?}\\ % The assignment title
	\vspace{12pt} % Whitespace
	\rule{\linewidth}{2pt}\\ % Thick bottom horizontal rule
	\vspace{12pt} % Whitespace
}

\author{\LARGE Jérôme Petit} % Your name

\date{\normalsize\today} % Today's date (\today) or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	FIGURE EXAMPLE
%----------------------------------------------------------------------------------------
%
\section{Introduction}


\section{Séries temporelles}
Une série temporelle ou série chronologique est une suite d’observations d’un phénomène faites au cours du temps. Cette notion s'étend à de nombreux domaines, tout relevé de données, de flux ou d'information est une série temporelle : indice boursier, population française, trafic Internet, etc. Ses données réelles obtenues sont des suites finies indexés par un temps continu ou discret. Les modèles mathématiques utilisées pour modélisées ses séries temporelles sont eux de dimension infinie.
\subsection{Stationnarité}
\begin{Def}
Stationnarité stricte ou forte~:~$\left(X_t\right)_{t\in\mathbb{Z}}$ est un processus stationnaire au sens strict si~:~$\forall n\in \mathbb{N}, \forall (t_1,...,t_n), \forall h\in\mathbb{Z}$ la loi de $(X_{t_1},...,X_{t_n})$ est identique à la loi de $(X_{t_1+h},...,X_{t_n+h})$
\end{Def}
D'après le théorème de Kolmogorov, on en déduit qu'un processus $\left(X_t\right)$ est stationnaire au sens fort si et seulement si la loi de $X_t$ est identique à la loi de $X_{t+h}$ quelque soit $h$. Cette condition de stationnarité est très contraignante et relativement peu observée en pratique. On remarque pour un processus stationnaire strict $X_t$ admettant des moments d'ordre 1 et 2 finis, on a~:
$$
\mathbb{E}\left(X_t\right)=\mathbb{E}\left(X_{t+h}\right), \forall h\in \mathbb{N}
$$
on pose~:~$\mathbb{E}\left(X_t\right)=\mathbb{E}\left(X_0\right)=\mu$. Pour la covariance, on a~:
\begin{align*}
Cov\left(X_t,X_{t+h}\right)& = \mathbb{E}\left((X_t-\mu)(X_{t+h}-\mu)\right)\\
&= \mathbb{E}\left((X_0-\mu)(X_{h_0}-\mu)\right)
\end{align*}
la dernière égalité vient du fait que $(X_t,X_{t+h})$ a la même distribution que $(X_0,X_h)$.
On remarque donc que les processus stationnaire strict ont des moments d'ordre 1 et 2 indépendant du temps $t$. C'est cette notion qui va être utilisée pour des définir les processus stationnaires au sens faible.
\begin{Def}
Stationnarité faible~:~
$\left(X_t\right)_{t\in\mathbb{Z}}$ est un processus stationnaire du second ordre (ou processus faiblement stationnaire ) s'il vérifie~:
\begin{itemize}
\item[i)]$\forall t\in\mathbb{Z},~\mathbb{E}\left(X_t\right)=m $ 
\item[ii)]$\forall t\in\mathbb{Z},~\mathbb{V}\left(X_t\right)=\sigma^2 $ 
\item[iii)]$\forall t,h\in\mathbb{Z},~Cov\left(X_t,X_{t+h}\right)=Cov\left(X_0,X_h\right)=\gamma\left(h\right) $ 
\end{itemize}
On note $\gamma(h)$ l'autocovariance d'ordre $h$ de $X_t$ .
\end{Def} 
Dans la suite les processus stationnaires désigneront les processus stationnaire du second ordre. La fonction d'autocovariance est paire et $\gamma(0)=\var{X_t}$.
\begin{Def}
Le coefficient d'autocorrélation d'ordre $h$ d'un processus stationnaire $\Xt$ est~:
$$\rho(h)=\frac{\gamma(h)}{\gamma(0)}.$$
\end{Def}
La fonction $h\mapsto\rho(h)$ est appelée fonction d'autocorrelation.
Parmi les processus stationnaires, il y a les processus appelés bruit blanc.
\begin{Def}
Un bruit blanc $\epsilon_t$ est une suite de variable aléatoire non corrélées, mais pas nécessairement indépendantes de moyenne nulle et de variance constante $\sigma^2$. On note $\epsilon_t\sim \bb{\sigma^2}$. 
\end{Def}
Un bruit blanc est donc un processus stationnaire d'ordre 2 tel que~:
\begin{itemize}
\item $\E{X_t}=0$\\
\item $\gamma(h)=0$ si $h\not =0$
\end{itemize}
Un bruit blanc n'est pas nécessairement fortement stationnaire car les variables aléatoires ne sont pas nécessairement indépendantes.
\begin{Def}
Un bruit blanc gaussien $\epsilon_t$ est une suite de variable aléatoire i.i.d de loi normale $\mathcal{N}\left(0,\sigma^2\right)$. On note $\epsilon\sim \bbn{\sigma^2}$
\end{Def}
Un bruit blanc gaussien est un processus stationnaire au sens strict. 


Les processus non stationnaires sont des processus dont les moments d'ordre 1 ou 2 ne sont pas un constant. La marche aléatoire est un exemple de processus non stationnaire car la variance est fonction du temps.

Dans les cas pratiques, on aura une série observée pour $t=1,...,T$, dans ce cas là on devra calculer l'autocavariance (resp. autocorrelation) empirique~:

\begin{align*}
\overline{\gamma}(h)&=\frac{1}{T}\sum_{t=h+1}^T(y_t-\overline{y})(y_{t-h}-\overline{y}), 0\leq h\leq T-1\\
\overline{\rho}(h)&=\frac{\sum_{t=h+1}^T(y_t-\overline{y})(y_{t-h}-\overline{y})}{\sum_{t=1}^T(y_t-\overline{y})^2}, 0\leq h\leq T-1,
\overline{y} &= \frac{1}{T}\sum_{t=1}^Ty_t
\end{align*}
Dans le cas ou le processus $y_t$ est stationnaire l
\subsection{Outil pour l'étude des processus stationnaires}
\begin{theorem}
Soit $X_t$ un processus stationnaire alors il existe une bruit blanc au sens faible $\epsilon_t$ et des coefficients réels $\left(\Psi_t\right)$ tel que~:
$$
X_t = m+\sum_{i\geq 0}\Psi_j\epsilon_j
$$
avec le moment d'ordre 1 de $X_t$
\end{theorem}
Cette décomposition est appelé décomposition de Wold. ou transformé par moyenne mobile.
Afin d'étudier les processus stationnaires, on utilisera les coefficients d'auto-corrélation et les coefficients d'auto corrélation partielles.
\begin{Def}
On appelle auto-corrélation partielle d'ordre p~:
$$
r(p)=\frac{Cov\left(X_t-\mathbb{E}\left(X_t|X_{t-1},...,X_{t-p}\right),X_{t-p}-\mathbb{E}\left(X_{t-p}|X_{t-1},...,X_{t-p+1}\right)\right)}{\mathbb{V}\left(X_t|X_{t-1},...,X_{t-p}\right)\mathbb{V}\left(X_{t-p}|X_{t-1},...,X_{t-p+1}\right)}
$$
\end{Def}

\section{Processus}
\subsection{processus MA}
\begin{Def}
Le processus $\Xt$ est une moyenne mobile d'ordre q s'il existe un bruit blanc $\epsilon_t\sim BB(0,\sigma^2)$ et des réels $\theta_1,...,\theta_q$ tels que~:~
$$
X_t=m+\epsilon_t+\theta_1\epsilon_{t-1}+...+\theta_q\epsilon_{t-q}
$$ 
On note MA(q) une moyenne mobile d'ordre q.
\end{Def}
Un processus MA(q) est stationnaire, la fonction d'auto-covariance est ~:
$$
\gamma(h)=\left\{\begin{array}{cc}
0 & \textrm{si |h|>q}\\
\sigma^2(1+\sum_{i=1}^q\theta^2_i & h=0\\
\sigma^2 \theta^2_q & h=q\\
\sigma^2(\theta_h+\sum_{i=h+1}^q\theta_i\theta_{i-h} & si 0<h<q\\
\end{array}
\right.
$$
Il n'y a pas de résultats particulier pour les auto-corrélations partielles.
\subsection{processus AR}
\begin{Def}
Un processus $\Xt$ est un processus auto-régressif d'ordre p noté AR(p) si~:
\begin{itemize}
\item[i)] $\Xt$ est stationnaire
\item[ii)] $\Xt$ vérifie~:
$$
X_t = \mu + \phi_1X_{t-1}+...+\phi_pX_{t-p}+\epsilon_t
$$
avec $\phi_p\not=0$ et $\epsilon_t\sim BB(0,\sigma^2)$
\end{itemize}
\end{Def}
On pose $\Phi(z)=1-\phi_1z-...-\phi_pz^p$, un processus AR(p) peut s'écrire à l'aide de polynôme de retard~:
$$
\Phi(L)X_t = mu + \epsilon_t
$$
A l'aide de cette formulation, on en déduit les racines du polynôme $\Phi$ sont toutes de module strictement supérieur à 1.
\begin{pro}
Soit $\Xt$ un processus AR(p), alors~:
\begin{itemize}
\item $\mathbb{E}\left(X_t\right) = \frac{\mu}{1-\phi_1-...-\phi_p}$\\
\item on pose $Y_t=X_t-\mathbb{E}\left(X_t\right)$ alors $Y_t$ est un processus AR(p) d'espérance nulle
\end{itemize}
\end{pro}
Les processus AR(p) sont des processus stationnaires, on a vu que les racines du polynômes de retard sont toutes strictement supérieure à 1, cela signifie que l'on peut inverser le polynôme de retard. Ainsi tout processus AR(p) peut s'écrire : $X_t = \Phi(L)^{-1}\left(\mu + \epsilon_t\right)$. L'inverse donnant une somme infinie, cela veut dire que tout processus AR(p) peut s'écrire sous la forme MA($\infty$).

\subsubsection{Propriétés des processus AR(p)}
On a vu précédemment que d'un processus AR(p) admettant une espérance non nulle, on pouvait toujours se ramenait à un processus AR(p) d'espérance nulle. Dans la suite on considérera le processus AR(p) $\Xt$ d'espérance nulle. L'auto covariance s'écrit pour tout h>0 : 
\begin{align*}
\gamma(h)&= Cov(X_t,X_{t-h})\\
&= \mathbb{E}\left(X_tX_{t-h}\right)\\
&= \sum_{i=1}^p\phi_i\mathbb{E}\left(X_{t-i}X_{t-h}\right)+\mathbb{E}\left(\epsilon_tX_{t-h}\right)\\
&= \sum_{i=1}^p\phi_i\gamma(h-i)
\end{align*}

Pour h=0, on a~:
\begin{align*}
\gamma(0)&= \mathbb{E}\left(X_t^2\right)\\
&= \sum_{i=1}^p\phi_i\mathbb{E}\left(X_{t-i}X_{t}\right)+\mathbb{E}\left(\epsilon_tX_{t}\right)\\
&= \sum_{i=1}^p\phi_i\gamma(i)+\mathbb{E}\left(\epsilon_t\epsilon_{t}\right)\\
&=\sum_{i=1}^p\phi_i\gamma(i)+\sigma^2
\end{align*}
On a de même pour l'auto-corrélation~:~$\rho(h)=\phi_1\rho(h-1)+...+\phi_p\rho(h-p)$ pour tout h>0.
Ces équations sont appelés equation de Yule-Walker. 
On a une relation de récurrence~:
$$
r(0)=1=\sum_{i=1}^p\phi_i\rho(i)+\frac{\sigma^2}{\gamma(0)}
$$
d'où~:
$$
\gamma(0)=\frac{\sigma^2}{1-\sum_{i=1}^p\phi_i\rho(i)}
$$
Les equations de Yule-Walker peuvent s'écrire~:
$$
\left(
\begin{array}{cccc}
1 & \rho(1) & ... & \rho(p-1)\\
\rho(1) & 1 & ... &  \rho(p-2)\\
\rho(2) & \rho(1) & ... & \rho(p-3)\\
.. & ... & ... & ...\\
\rho(p-1) & \rho(p-1) & ... & 1
\end{array}
\right)
\left(
\begin{array}{c}
\phi_1\\
. \\
. \\
.\\
\phi_p
\end{array}
\right)=\left(
\begin{array}{c}
\rho(1)\\
. \\
. \\
.\\
\rho(p)
\end{array}
\right)
$$
Les solutions de ce système sont données par les valeurs initiales $\rho(i)$. Ainsi si l'on peut estimer les corrélation sur un échantillon donnée on pourra en déduire les coefficients du processus AR(p).
\begin{pro}
Soit $\Xt$ un processus AR(p) alors~:
\begin{itemize}
\item[i)] $|\rho(h)|$ et $\gamma(h)$ décroissent exponentiellement avec h\\
\item[ii)] l'auto corrélation partielle est nulle pour h>p
\end{itemize}
\end{pro}
\subsection{ARMA modèles}
\subsection{SARMA modèles}
\subsection{ARIMA modèles}
\subsection{SARIMA modèles}
\subsection{ARCH modèles}
\subsection{GARCH modèles}

\section{Annexe}
\subsection{Convergence}
Convergence $\mathcal{L}^2$~:
$$
X_n\rightarrow_{\mathcal{L^2}} X \Leftrightarrow  \lim_{n\rightarrow \infty}||X_n-X||_2=0
$$
On dit que $X_t$ convergent en loi vers $X$ si et seulement si pour toute fonction bornée $\phi$, on a~:~
$$
\lim_{n}\mathcal{E}(\phi(X_n))=\mathcal{E}\left(\phi(X)\right)
$$
\subsection{Densité spectrale}
\begin{pro}
Soit $(X_t)$ un processus stationnaire de la forme~:
$$
X_t = m+\sum_{j\geq 0}a_j\epsilon_{t-j}
$$
avec $\epsilon_t$ un bruit blanc $BB(0,\sigma^2)$ et $\sum_{j\geq 0}|a_j|<+\infty$. Alors~:
\begin{itemize}
\item $\sum_{h\in\mathbb{Z}}|\gamma(h)|<+\infty$\\
\item $\forall \omega\in[-\pi,\pi],~f_X(\omega)=\frac{1}{2\pi}\sum_{h\in\mathbb{Z}}\gamma(h)e^{i\omega h}$ est la densité spectrale de $X_t$.
\end{itemize}
\end{pro}
Sous les hypothèses précédentes, on a~:
$$
f_X(\omega)=\frac{1}{2\pi}\sum_{h\in\mathbb{Z}}\gamma(h)cos(\omega h)
$$
\subsection{Estimateur empirique des moments}
Soit $(X_t)_{t\in\mathbb{Z}}$ un processus stationnaire, on cherche à estimer l'espérance, les fonction d'auto-covariance, d'auto-corrélation et d'auto-corrélation partielle. On a un échantillon de taille T. On prend comme estimateur~:
\begin{itemize}
\item moyenne empirique~:~$\hat{m}=\frac{1}{T}\sum_{t=1}^TX_t=\overline{X}_T$\\
\item auto-covariance empirique d'ordre h~:~$\hat{\gamma}(h)=\frac{1}{T-h}\sum_{t=h+1}^T(X_t-\overline{X}_T)(X_{t-h}-\overline{X}_T)$\\
\item auto-corrélation empirique d'ordre h~:~$\hat{\rho}(h)=\frac{\hat{\gamma}(h)}{\hat{\gamma}(0)}$.\\
\item l'auto-corrélation partielle est une régression empirique sur 1,$X_1$,...,$X-{t-h}$.
\end{itemize}
Pour estimer la fonction de densité spectrale on utilise la auto-covariance empirique et un terme d'ajustement (coefficient de Newey-West) afin diminuer le poids de $\hat{\gamma}(h)$ pour des h grand.


Si $X_t$ est un processus stationnaire alors d'après la loi des grands nombres tous les estimateurs empiriques présentés ci-dessus convergent.

\begin{pro}
Si $X_t=m+\sum_{j\geq 0}a_j\epsilon_{t-j}$ avec $\epsilon_t$ un bruit blanc de moment d'ordre 4 fini, alors tous ses estimateurs ont des lois jointes asymptotiquement gaussiennes~:
$$
\sqrt{T}\left(\hat{m}-m\right)\rightarrow \mathcal{N}(0,\sum_{h\in\mathcal{Z}}\gamma(h)
$$
convergence en loi
\end{pro}
\subsection{polynôme de retard}
\begin{Def}
L'opérateur de retard est défini sur la classe des processus stationnaires comme étant~: 
$$
L(X_t)=X_{t-1}
$$
\end{Def}
\begin{Def}
Soit $P$ un polynôme, $P(z)=\sum_{k=0}^{p}a_kz^k$ avec $a_k\in\mathbb{R}$, on lui associe le polynôme de retard $P(L)$ défini comme suit~:
$$
P(L)=\sum_{k=0}^pa_kL^k
$$
et 
$$
P(L)X_t=\sum_{k=0}^pa_kX_{t-k}
$$
\end{Def}
Un polynôme de retard P(L) est inversible s'il existe un polynôme de retard $Q(L)$ tel que $P(L)Q(L)=Id$. Dans $\mathbb{C}$, tout polynôme réel $P$ se décompose~:
$$
P(z)=\prod_{i=1}^p(z-z_i)=\prod_{i=1}^p(-z_i)\prod_{i=1}^p(1-\frac{z}{z_i})=\alpha\prod_{i=1}^p(1-\lambda_i z)
$$
avec $\lambda_i=\frac{1}{z_i}$.
Le polynôme est $(1-\lambda z)$ est inversible si seulement si ses racines sont de modules strictement inférieur à 1. Ceci équivalent à $|\lambda|>1$. On peut généraliser ce résultat à tout polynôme de degré p.
\begin{pro}
Soit $P(L) = \alpha\prod_{i=1}^p(1-\lambda_i L)$ un polynôme de retard, si pour tout i $|\lambda_i|<1$ alors $P(L)$ est inversible et :
$$
P^{-1}(L)= \sum_{k\geq 0}b_kL^k
$$
\end{pro}
\end{document}
